[[1;34mINFO[m] Scanning for projects...
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m-----------------------< [0;36mit.unipi.hadoop:KMeans[0;1m >-----------------------[m
[[1;34mINFO[m] [1mBuilding kmeans 1.0-SNAPSHOT[m
[[1;34mINFO[m] [1m--------------------------------[ jar ]---------------------------------[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-clean-plugin:2.5:clean[m [1m(default-clean)[m @ [36mKMeans[0;1m ---[m
[[1;34mINFO[m] Deleting /home/hadoop/CloudProgrammingTonellotto/kmeans/target
[[1;34mINFO[m] [1m------------------------------------------------------------------------[m
[[1;34mINFO[m] [1;32mBUILD SUCCESS[m
[[1;34mINFO[m] [1m------------------------------------------------------------------------[m
[[1;34mINFO[m] Total time:  0.738 s
[[1;34mINFO[m] Finished at: 2020-06-30T18:49:00+01:00
[[1;34mINFO[m] [1m------------------------------------------------------------------------[m
[[1;34mINFO[m] Scanning for projects...
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m-----------------------< [0;36mit.unipi.hadoop:KMeans[0;1m >-----------------------[m
[[1;34mINFO[m] [1mBuilding kmeans 1.0-SNAPSHOT[m
[[1;34mINFO[m] [1m--------------------------------[ jar ]---------------------------------[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:resources[m [1m(default-resources)[m @ [36mKMeans[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] skip non existing resourceDirectory /home/hadoop/CloudProgrammingTonellotto/kmeans/src/main/resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.0:compile[m [1m(default-compile)[m @ [36mKMeans[0;1m ---[m
[[1;34mINFO[m] Changes detected - recompiling the module!
[[1;34mINFO[m] Compiling 5 source files to /home/hadoop/CloudProgrammingTonellotto/kmeans/target/classes
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:testResources[m [1m(default-testResources)[m @ [36mKMeans[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] skip non existing resourceDirectory /home/hadoop/CloudProgrammingTonellotto/kmeans/src/test/resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.0:testCompile[m [1m(default-testCompile)[m @ [36mKMeans[0;1m ---[m
[[1;34mINFO[m] No sources to compile
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-surefire-plugin:2.12.4:test[m [1m(default-test)[m @ [36mKMeans[0;1m ---[m
[[1;34mINFO[m] No tests to run.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-jar-plugin:3.1.0:jar[m [1m(default-jar)[m @ [36mKMeans[0;1m ---[m
[[1;34mINFO[m] Building jar: /home/hadoop/CloudProgrammingTonellotto/kmeans/target/KMeans-1.0-SNAPSHOT.jar
[[1;34mINFO[m] [1m------------------------------------------------------------------------[m
[[1;34mINFO[m] [1;32mBUILD SUCCESS[m
[[1;34mINFO[m] [1m------------------------------------------------------------------------[m
[[1;34mINFO[m] Total time:  11.704 s
[[1;34mINFO[m] Finished at: 2020-06-30T18:49:17+01:00
[[1;34mINFO[m] [1m------------------------------------------------------------------------[m
2020-06-30 18:49:34,481 INFO client.RMProxy: Connecting to ResourceManager at hadoop-namenode/172.16.1.142:8032
2020-06-30 18:49:34,627 INFO client.RMProxy: Connecting to ResourceManager at hadoop-namenode/172.16.1.142:8032
2020-06-30 18:49:34,815 INFO client.RMProxy: Connecting to ResourceManager at hadoop-namenode/172.16.1.142:8032
2020-06-30 18:49:35,224 INFO client.RMProxy: Connecting to ResourceManager at hadoop-namenode/172.16.1.142:8032
2020-06-30 18:49:35,225 INFO client.RMProxy: Connecting to ResourceManager at hadoop-namenode/172.16.1.142:8032
2020-06-30 18:49:35,351 INFO client.RMProxy: Connecting to ResourceManager at hadoop-namenode/172.16.1.142:8032
2020-06-30 18:49:35,718 INFO client.RMProxy: Connecting to ResourceManager at hadoop-namenode/172.16.1.142:8032
2020-06-30 18:49:35,790 INFO client.RMProxy: Connecting to ResourceManager at hadoop-namenode/172.16.1.142:8032
2020-06-30 18:49:35,860 INFO client.RMProxy: Connecting to ResourceManager at hadoop-namenode/172.16.1.142:8032
2020-06-30 18:49:36,034 INFO client.RMProxy: Connecting to ResourceManager at hadoop-namenode/172.16.1.142:8032
2020-06-30 18:49:36,045 INFO client.RMProxy: Connecting to ResourceManager at hadoop-namenode/172.16.1.142:8032
2020-06-30 18:49:36,420 INFO client.RMProxy: Connecting to ResourceManager at hadoop-namenode/172.16.1.142:8032
2020-06-30 18:49:38,635 WARN mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2020-06-30 18:49:38,829 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1593532013608_0176
2020-06-30 18:49:39,150 WARN mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2020-06-30 18:49:39,175 WARN mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2020-06-30 18:49:39,346 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1593532013608_0178
2020-06-30 18:49:39,363 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1593532013608_0177
2020-06-30 18:49:39,505 WARN mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2020-06-30 18:49:39,564 WARN mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2020-06-30 18:49:39,572 WARN mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2020-06-30 18:49:39,629 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1593532013608_0181
2020-06-30 18:49:39,751 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1593532013608_0180
2020-06-30 18:49:39,782 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1593532013608_0179
2020-06-30 18:49:40,135 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:49:40,287 WARN mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2020-06-30 18:49:40,444 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1593532013608_0182
2020-06-30 18:49:40,528 WARN mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2020-06-30 18:49:40,547 WARN mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2020-06-30 18:49:40,585 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:49:40,736 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1593532013608_0184
2020-06-30 18:49:40,746 WARN mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2020-06-30 18:49:40,770 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1593532013608_0183
2020-06-30 18:49:40,798 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:49:40,930 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1593532013608_0185
2020-06-30 18:49:40,965 WARN mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2020-06-30 18:49:41,045 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:49:41,082 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1593532013608_0186
2020-06-30 18:49:41,123 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:49:41,123 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:49:41,357 WARN mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2020-06-30 18:49:41,524 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1593532013608_0187
2020-06-30 18:49:41,745 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:49:41,882 INFO mapreduce.JobSubmitter: Cleaning up the staging area /tmp/hadoop-yarn/staging/hadoop/.staging/job_1593532013608_0176
Exception in thread "main" org.apache.hadoop.mapreduce.lib.input.InvalidInputException: Input path does not exist: hdfs://hadoop-namenode:9820/user/hadoop/input_data_d=3_n=100000_k=7.txt
	at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:332)
	at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.listStatus(FileInputFormat.java:274)
	at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.getSplits(FileInputFormat.java:396)
	at org.apache.hadoop.mapreduce.JobSubmitter.writeNewSplits(JobSubmitter.java:310)
	at org.apache.hadoop.mapreduce.JobSubmitter.writeSplits(JobSubmitter.java:327)
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:200)
	at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1570)
	at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1567)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:1567)
	at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1588)
	at it.unipi.hadoop.Driver.getRandomCentroids(Driver.java:59)
	at it.unipi.hadoop.Driver.main(Driver.java:210)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:318)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:232)
2020-06-30 18:49:42,026 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:49:42,031 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:49:42,234 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:49:42,236 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:49:42,303 INFO input.FileInputFormat: Total input files to process : 1
2020-06-30 18:49:42,651 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:49:42,698 INFO mapreduce.JobSubmitter: Cleaning up the staging area /tmp/hadoop-yarn/staging/hadoop/.staging/job_1593532013608_0177
2020-06-30 18:49:42,714 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
Exception in thread "main" org.apache.hadoop.mapreduce.lib.input.InvalidInputException: Input path does not exist: hdfs://hadoop-namenode:9820/user/hadoop/input_data_d=3_n=1000_k=7.txt
	at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:332)
2020-06-30 18:49:42,756 INFO input.FileInputFormat: Total input files to process : 1
	at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.listStatus(FileInputFormat.java:274)
	at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.getSplits(FileInputFormat.java:396)
	at org.apache.hadoop.mapreduce.JobSubmitter.writeNewSplits(JobSubmitter.java:310)
	at org.apache.hadoop.mapreduce.JobSubmitter.writeSplits(JobSubmitter.java:327)
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:200)
	at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1570)
	at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1567)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:1567)
	at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1588)
	at it.unipi.hadoop.Driver.getRandomCentroids(Driver.java:59)
	at it.unipi.hadoop.Driver.main(Driver.java:210)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:318)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:232)
2020-06-30 18:49:42,868 INFO input.FileInputFormat: Total input files to process : 1
2020-06-30 18:49:42,899 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:49:42,953 INFO mapreduce.JobSubmitter: Cleaning up the staging area /tmp/hadoop-yarn/staging/hadoop/.staging/job_1593532013608_0179
2020-06-30 18:49:42,966 INFO mapreduce.JobSubmitter: number of splits:1
Exception in thread "main" org.apache.hadoop.mapreduce.lib.input.InvalidInputException: Input path does not exist: hdfs://hadoop-namenode:9820/user/hadoop/input_data_d=7_n=1000_k=7.txt
	at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:332)
	at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.listStatus(FileInputFormat.java:274)
	at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.getSplits(FileInputFormat.java:396)
	at org.apache.hadoop.mapreduce.JobSubmitter.writeNewSplits(JobSubmitter.java:310)
	at org.apache.hadoop.mapreduce.JobSubmitter.writeSplits(JobSubmitter.java:327)
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:200)
	at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1570)
	at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1567)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:1567)
	at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1588)
	at it.unipi.hadoop.Driver.getRandomCentroids(Driver.java:59)
	at it.unipi.hadoop.Driver.main(Driver.java:210)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:318)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:232)
2020-06-30 18:49:43,093 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:49:43,275 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:49:43,281 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:49:43,383 INFO mapreduce.JobSubmitter: number of splits:1
2020-06-30 18:49:43,453 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:49:43,513 INFO mapreduce.JobSubmitter: number of splits:1
2020-06-30 18:49:43,558 INFO mapreduce.JobSubmitter: Cleaning up the staging area /tmp/hadoop-yarn/staging/hadoop/.staging/job_1593532013608_0183
2020-06-30 18:49:43,572 INFO mapreduce.JobSubmitter: Cleaning up the staging area /tmp/hadoop-yarn/staging/hadoop/.staging/job_1593532013608_0182
Exception in thread "main" org.apache.hadoop.mapreduce.lib.input.InvalidInputException: Input path does not exist: hdfs://hadoop-namenode:9820/user/hadoop/input_data_d=7_n=100000_k=7.txt
	at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:332)
	at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.listStatus(FileInputFormat.java:274)Exception in thread "main" org.apache.hadoop.mapreduce.lib.input.InvalidInputException: Input path does not exist: hdfs://hadoop-namenode:9820/user/hadoop/input_data_d=3_n=10000_k=7.txt

	at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.getSplits(FileInputFormat.java:396)
	at org.apache.hadoop.mapreduce.JobSubmitter.writeNewSplits(JobSubmitter.java:310)
	at org.apache.hadoop.mapreduce.JobSubmitter.writeSplits(JobSubmitter.java:327)
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:200)
	at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1570)
	at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1567)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:1567)
	at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1588)
	at it.unipi.hadoop.Driver.getRandomCentroids(Driver.java:59)
	at it.unipi.hadoop.Driver.main(Driver.java:210)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:318)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:232)
	at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:332)
	at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.listStatus(FileInputFormat.java:274)
	at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.getSplits(FileInputFormat.java:396)
	at org.apache.hadoop.mapreduce.JobSubmitter.writeNewSplits(JobSubmitter.java:310)
	at org.apache.hadoop.mapreduce.JobSubmitter.writeSplits(JobSubmitter.java:327)
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:200)
	at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1570)
	at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1567)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:1567)
	at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1588)
	at it.unipi.hadoop.Driver.getRandomCentroids(Driver.java:59)
	at it.unipi.hadoop.Driver.main(Driver.java:210)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:318)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:232)
2020-06-30 18:49:43,647 INFO mapreduce.JobSubmitter: Cleaning up the staging area /tmp/hadoop-yarn/staging/hadoop/.staging/job_1593532013608_0184
Exception in thread "main" org.apache.hadoop.mapreduce.lib.input.InvalidInputException: Input path does not exist: hdfs://hadoop-namenode:9820/user/hadoop/input_data_d=7_n=10000_k=7.txt
	at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:332)
	at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.listStatus(FileInputFormat.java:274)
	at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.getSplits(FileInputFormat.java:396)
	at org.apache.hadoop.mapreduce.JobSubmitter.writeNewSplits(JobSubmitter.java:310)
	at org.apache.hadoop.mapreduce.JobSubmitter.writeSplits(JobSubmitter.java:327)
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:200)
	at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1570)
	at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1567)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:1567)
	at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1588)
	at it.unipi.hadoop.Driver.getRandomCentroids(Driver.java:59)
	at it.unipi.hadoop.Driver.main(Driver.java:210)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:318)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:232)
2020-06-30 18:49:43,756 INFO input.FileInputFormat: Total input files to process : 1
2020-06-30 18:49:43,943 INFO input.FileInputFormat: Total input files to process : 1
2020-06-30 18:49:44,011 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:49:44,058 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:49:44,116 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1593532013608_0178
2020-06-30 18:49:44,118 INFO mapreduce.JobSubmitter: Executing with tokens: []
2020-06-30 18:49:44,147 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:49:44,160 INFO input.FileInputFormat: Total input files to process : 1
2020-06-30 18:49:44,168 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:49:44,236 INFO mapreduce.JobSubmitter: number of splits:1
2020-06-30 18:49:44,343 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:49:44,385 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:49:44,399 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:49:44,400 INFO mapreduce.JobSubmitter: number of splits:1
2020-06-30 18:49:44,417 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:49:44,562 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1593532013608_0181
2020-06-30 18:49:44,563 INFO mapreduce.JobSubmitter: Executing with tokens: []
2020-06-30 18:49:44,569 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:49:44,589 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1593532013608_0180
2020-06-30 18:49:44,589 INFO mapreduce.JobSubmitter: Executing with tokens: []
2020-06-30 18:49:44,636 INFO mapreduce.JobSubmitter: number of splits:1
2020-06-30 18:49:44,968 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:49:45,064 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1593532013608_0186
2020-06-30 18:49:45,065 INFO mapreduce.JobSubmitter: Executing with tokens: []
2020-06-30 18:49:45,226 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:49:45,328 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:49:45,369 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1593532013608_0185
2020-06-30 18:49:45,370 INFO mapreduce.JobSubmitter: Executing with tokens: []
2020-06-30 18:49:45,454 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1593532013608_0187
2020-06-30 18:49:45,455 INFO mapreduce.JobSubmitter: Executing with tokens: []
2020-06-30 18:49:45,533 INFO conf.Configuration: resource-types.xml not found
2020-06-30 18:49:45,545 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.
2020-06-30 18:49:45,824 INFO conf.Configuration: resource-types.xml not found
2020-06-30 18:49:45,826 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.
2020-06-30 18:49:45,912 INFO impl.YarnClientImpl: Submitted application application_1593532013608_0178
2020-06-30 18:49:45,976 INFO conf.Configuration: resource-types.xml not found
2020-06-30 18:49:45,984 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.
2020-06-30 18:49:46,172 INFO mapreduce.Job: The url to track the job: http://hadoop-namenode:8088/proxy/application_1593532013608_0178/
2020-06-30 18:49:46,174 INFO mapreduce.Job: Running job: job_1593532013608_0178
2020-06-30 18:49:46,215 INFO impl.YarnClientImpl: Submitted application application_1593532013608_0181
2020-06-30 18:49:46,264 INFO conf.Configuration: resource-types.xml not found
2020-06-30 18:49:46,272 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.
2020-06-30 18:49:46,360 INFO impl.YarnClientImpl: Submitted application application_1593532013608_0180
2020-06-30 18:49:46,431 INFO mapreduce.Job: The url to track the job: http://hadoop-namenode:8088/proxy/application_1593532013608_0181/
2020-06-30 18:49:46,436 INFO mapreduce.Job: Running job: job_1593532013608_0181
2020-06-30 18:49:46,553 INFO conf.Configuration: resource-types.xml not found
2020-06-30 18:49:46,556 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.
2020-06-30 18:49:46,591 INFO mapreduce.Job: The url to track the job: http://hadoop-namenode:8088/proxy/application_1593532013608_0180/
2020-06-30 18:49:46,595 INFO mapreduce.Job: Running job: job_1593532013608_0180
2020-06-30 18:49:46,600 INFO impl.YarnClientImpl: Submitted application application_1593532013608_0186
2020-06-30 18:49:46,693 INFO conf.Configuration: resource-types.xml not found
2020-06-30 18:49:46,693 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.
2020-06-30 18:49:46,699 INFO mapreduce.Job: The url to track the job: http://hadoop-namenode:8088/proxy/application_1593532013608_0186/
2020-06-30 18:49:46,700 INFO mapreduce.Job: Running job: job_1593532013608_0186
2020-06-30 18:49:46,715 INFO impl.YarnClientImpl: Submitted application application_1593532013608_0185
2020-06-30 18:49:46,850 INFO mapreduce.Job: The url to track the job: http://hadoop-namenode:8088/proxy/application_1593532013608_0185/
2020-06-30 18:49:46,855 INFO mapreduce.Job: Running job: job_1593532013608_0185
2020-06-30 18:49:46,918 INFO impl.YarnClientImpl: Submitted application application_1593532013608_0187
2020-06-30 18:49:47,029 INFO mapreduce.Job: The url to track the job: http://hadoop-namenode:8088/proxy/application_1593532013608_0187/
2020-06-30 18:49:47,034 INFO mapreduce.Job: Running job: job_1593532013608_0187
2020-06-30 18:49:51,711 INFO mapreduce.Job: Job job_1593532013608_0180 running in uber mode : false
2020-06-30 18:49:51,712 INFO mapreduce.Job:  map 0% reduce 0%
2020-06-30 18:49:51,780 INFO mapreduce.Job: Job job_1593532013608_0186 running in uber mode : false
2020-06-30 18:49:51,781 INFO mapreduce.Job:  map 0% reduce 0%
2020-06-30 18:49:53,444 INFO mapreduce.Job: Job job_1593532013608_0178 running in uber mode : false
2020-06-30 18:49:53,445 INFO mapreduce.Job:  map 0% reduce 0%
2020-06-30 18:49:54,972 INFO mapreduce.Job: Job job_1593532013608_0185 running in uber mode : false
2020-06-30 18:49:54,973 INFO mapreduce.Job:  map 0% reduce 0%
2020-06-30 18:49:56,854 INFO mapreduce.Job:  map 100% reduce 0%
2020-06-30 18:49:57,608 INFO mapreduce.Job: Job job_1593532013608_0181 running in uber mode : false
2020-06-30 18:49:57,609 INFO mapreduce.Job:  map 0% reduce 0%
2020-06-30 18:49:58,808 INFO mapreduce.Job:  map 100% reduce 0%
2020-06-30 18:49:59,533 INFO mapreduce.Job:  map 100% reduce 0%
2020-06-30 18:50:00,050 INFO mapreduce.Job:  map 100% reduce 0%
2020-06-30 18:50:01,649 INFO mapreduce.Job:  map 100% reduce 0%
2020-06-30 18:50:01,890 INFO mapreduce.Job:  map 100% reduce 100%
2020-06-30 18:50:01,900 INFO mapreduce.Job: Job job_1593532013608_0186 completed successfully
2020-06-30 18:50:02,013 INFO mapreduce.Job: Counters: 53
	File System Counters
		FILE: Number of bytes read=149
		FILE: Number of bytes written=436613
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=1373765
		HDFS: Number of bytes written=135
		HDFS: Number of read operations=8
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=1
		Launched reduce tasks=1
		Rack-local map tasks=1
		Total time spent by all maps in occupied slots (ms)=4620
		Total time spent by all reduces in occupied slots (ms)=5298
		Total time spent by all map tasks (ms)=2310
		Total time spent by all reduce tasks (ms)=2649
		Total vcore-milliseconds taken by all map tasks=2310
		Total vcore-milliseconds taken by all reduce tasks=2649
		Total megabyte-milliseconds taken by all map tasks=591360
		Total megabyte-milliseconds taken by all reduce tasks=678144
	Map-Reduce Framework
		Map input records=10000
		Map output records=10000
		Map output bytes=1423634
		Map output materialized bytes=149
		Input split bytes=131
		Combine input records=10000
		Combine output records=1
		Reduce input groups=1
		Reduce shuffle bytes=149
		Reduce input records=1
		Reduce output records=1
		Spilled Records=2
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=173
		CPU time spent (ms)=1180
		Physical memory (bytes) snapshot=443281408
		Virtual memory (bytes) snapshot=3838169088
		Total committed heap usage (bytes)=333447168
		Peak Map Physical memory (bytes)=273342464
		Peak Map Virtual memory (bytes)=1913860096
		Peak Reduce Physical memory (bytes)=169938944
		Peak Reduce Virtual memory (bytes)=1924308992
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=1373634
	File Output Format Counters 
		Bytes Written=135
2020-06-30 18:50:02,053 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
java.lang.NumberFormatException: multiple points
	at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:1914)
	at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110)
	at java.lang.Double.parseDouble(Double.java:538)
	at it.unipi.hadoop.Point.parseString(Point.java:51)
	at it.unipi.hadoop.Driver.getRandomCentroids(Driver.java:67)
	at it.unipi.hadoop.Driver.main(Driver.java:210)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:318)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:232)
2020-06-30 18:50:02,105 INFO client.RMProxy: Connecting to ResourceManager at hadoop-namenode/172.16.1.142:8032
2020-06-30 18:50:02,114 WARN mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2020-06-30 18:50:02,129 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1593532013608_0188
2020-06-30 18:50:02,153 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:50:02,213 INFO input.FileInputFormat: Total input files to process : 1
2020-06-30 18:50:02,249 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:50:02,328 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:50:02,358 INFO mapreduce.JobSubmitter: number of splits:1
2020-06-30 18:50:02,392 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:50:02,426 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1593532013608_0188
2020-06-30 18:50:02,426 INFO mapreduce.JobSubmitter: Executing with tokens: []
2020-06-30 18:50:02,444 INFO impl.YarnClientImpl: Submitted application application_1593532013608_0188
2020-06-30 18:50:02,447 INFO mapreduce.Job: The url to track the job: http://hadoop-namenode:8088/proxy/application_1593532013608_0188/
2020-06-30 18:50:02,447 INFO mapreduce.Job: Running job: job_1593532013608_0188
2020-06-30 18:50:03,564 INFO mapreduce.Job: Task Id : attempt_1593532013608_0178_r_000000_0, Status : FAILED
Error: org.apache.hadoop.ipc.RemoteException(java.io.FileNotFoundException): File does not exist: /user/hadoop/KMeansTemporaryUniformSampling/_temporary/1/_temporary/attempt_1593532013608_0178_r_000000_0/part-r-00000 (inode 35317) Holder DFSClient_attempt_1593532013608_0178_r_000000_0_-1861217230_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2840)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.completeFileInternal(FSDirWriteFileOp.java:693)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.completeFile(FSDirWriteFileOp.java:679)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:2883)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:947)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:614)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:527)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1036)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1000)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:928)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2916)

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1545)
	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
	at com.sun.proxy.$Proxy13.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:554)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy14.complete(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:949)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:907)
	at org.apache.hadoop.hdfs.DFSOutputStream.closeImpl(DFSOutputStream.java:890)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:845)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:101)
	at org.apache.hadoop.mapreduce.lib.output.TextOutputFormat$LineRecordWriter.close(TextOutputFormat.java:106)
	at org.apache.hadoop.mapred.ReduceTask$NewTrackingRecordWriter.close(ReduceTask.java:551)
	at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:630)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:390)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)

2020-06-30 18:50:04,853 INFO mapreduce.Job:  map 100% reduce 100%
2020-06-30 18:50:04,864 INFO mapreduce.Job: Job job_1593532013608_0180 completed successfully
2020-06-30 18:50:05,032 INFO mapreduce.Job: Counters: 53
	File System Counters
		FILE: Number of bytes read=72
		FILE: Number of bytes written=436427
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=592955
		HDFS: Number of bytes written=60
		HDFS: Number of read operations=8
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=1
		Launched reduce tasks=1
		Data-local map tasks=1
		Total time spent by all maps in occupied slots (ms)=9540
		Total time spent by all reduces in occupied slots (ms)=7330
		Total time spent by all map tasks (ms)=4770
		Total time spent by all reduce tasks (ms)=3665
		Total vcore-milliseconds taken by all map tasks=4770
		Total vcore-milliseconds taken by all reduce tasks=3665
		Total megabyte-milliseconds taken by all map tasks=1221120
		Total megabyte-milliseconds taken by all reduce tasks=938240
	Map-Reduce Framework
		Map input records=10000
		Map output records=10000
		Map output bytes=632824
		Map output materialized bytes=72
		Input split bytes=131
		Combine input records=10000
		Combine output records=1
		Reduce input groups=1
		Reduce shuffle bytes=72
		Reduce input records=1
		Reduce output records=1
		Spilled Records=2
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=221
		CPU time spent (ms)=1200
		Physical memory (bytes) snapshot=443129856
		Virtual memory (bytes) snapshot=3842142208
		Total committed heap usage (bytes)=330301440
		Peak Map Physical memory (bytes)=273022976
		Peak Map Virtual memory (bytes)=1918054400
		Peak Reduce Physical memory (bytes)=170106880
		Peak Reduce Virtual memory (bytes)=1924087808
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=592824
	File Output Format Counters 
		Bytes Written=60
2020-06-30 18:50:05,082 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:50:05,166 INFO client.RMProxy: Connecting to ResourceManager at hadoop-namenode/172.16.1.142:8032
2020-06-30 18:50:05,187 WARN mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2020-06-30 18:50:05,205 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1593532013608_0189
2020-06-30 18:50:05,241 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:50:05,290 INFO input.FileInputFormat: Total input files to process : 1
2020-06-30 18:50:05,342 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:50:05,416 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:50:05,433 INFO mapreduce.JobSubmitter: number of splits:1
2020-06-30 18:50:05,514 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:50:05,588 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1593532013608_0189
2020-06-30 18:50:05,589 INFO mapreduce.JobSubmitter: Executing with tokens: []
2020-06-30 18:50:05,620 INFO impl.YarnClientImpl: Submitted application application_1593532013608_0189
2020-06-30 18:50:05,624 INFO mapreduce.Job: The url to track the job: http://hadoop-namenode:8088/proxy/application_1593532013608_0189/
2020-06-30 18:50:05,624 INFO mapreduce.Job: Running job: job_1593532013608_0189
2020-06-30 18:50:05,680 INFO mapreduce.Job: Task Id : attempt_1593532013608_0181_r_000000_0, Status : FAILED
Error: java.io.FileNotFoundException: File does not exist: /user/hadoop/KMeansTemporaryUniformSampling/_temporary/1/_temporary/attempt_1593532013608_0181_r_000000_0/part-r-00000 (inode 35345) Holder DFSClient_attempt_1593532013608_0181_r_000000_0_1814134919_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2840)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.analyzeFileState(FSDirWriteFileOp.java:599)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.validateAddBlock(FSDirWriteFileOp.java:171)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2719)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:892)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:568)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:527)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1036)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1000)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:928)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2916)

	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:121)
	at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:88)
	at org.apache.hadoop.hdfs.DFSOutputStream.addBlock(DFSOutputStream.java:1084)
	at org.apache.hadoop.hdfs.DataStreamer.locateFollowingBlock(DataStreamer.java:1866)
	at org.apache.hadoop.hdfs.DataStreamer.nextBlockOutputStream(DataStreamer.java:1668)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:716)
Caused by: org.apache.hadoop.ipc.RemoteException(java.io.FileNotFoundException): File does not exist: /user/hadoop/KMeansTemporaryUniformSampling/_temporary/1/_temporary/attempt_1593532013608_0181_r_000000_0/part-r-00000 (inode 35345) Holder DFSClient_attempt_1593532013608_0181_r_000000_0_1814134919_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2840)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.analyzeFileState(FSDirWriteFileOp.java:599)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.validateAddBlock(FSDirWriteFileOp.java:171)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2719)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:892)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:568)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:527)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1036)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1000)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:928)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2916)

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1545)
	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
	at com.sun.proxy.$Proxy13.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:514)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy14.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.addBlock(DFSOutputStream.java:1081)
	... 3 more

2020-06-30 18:50:06,096 INFO mapreduce.Job:  map 100% reduce 100%
2020-06-30 18:50:06,107 INFO mapreduce.Job: Job job_1593532013608_0185 completed successfully
2020-06-30 18:50:06,211 INFO mapreduce.Job: Counters: 53
	File System Counters
		FILE: Number of bytes read=72
		FILE: Number of bytes written=436461
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=5930445
		HDFS: Number of bytes written=60
		HDFS: Number of read operations=8
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=3
	Job Counters 
		Launched map tasks=1
		Launched reduce tasks=1
		Rack-local map tasks=1
		Total time spent by all maps in occupied slots (ms)=5110
		Total time spent by all reduces in occupied slots (ms)=7326
		Total time spent by all map tasks (ms)=2555
		Total time spent by all reduce tasks (ms)=3663
		Total vcore-milliseconds taken by all map tasks=2555
		Total vcore-milliseconds taken by all reduce tasks=3663
		Total megabyte-milliseconds taken by all map tasks=654080
		Total megabyte-milliseconds taken by all reduce tasks=937728
	Map-Reduce Framework
		Map input records=100000
		Map output records=100000
		Map output bytes=6330313
		Map output materialized bytes=72
		Input split bytes=132
		Combine input records=100000
		Combine output records=1
		Reduce input groups=1
		Reduce shuffle bytes=72
		Reduce input records=1
		Reduce output records=1
		Spilled Records=2
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=138
		CPU time spent (ms)=1550
		Physical memory (bytes) snapshot=446754816
		Virtual memory (bytes) snapshot=3841687552
		Total committed heap usage (bytes)=319291392
		Peak Map Physical memory (bytes)=277446656
		Peak Map Virtual memory (bytes)=1917845504
		Peak Reduce Physical memory (bytes)=169308160
		Peak Reduce Virtual memory (bytes)=1923842048
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=5930313
	File Output Format Counters 
		Bytes Written=60
Exception in thread "main" java.io.FileNotFoundException: File hdfs://hadoop-namenode:9820/user/hadoop/KMeansTemporaryUniformSampling does not exist.
	at org.apache.hadoop.hdfs.DistributedFileSystem$DirListingIterator.<init>(DistributedFileSystem.java:1220)
	at org.apache.hadoop.hdfs.DistributedFileSystem$DirListingIterator.<init>(DistributedFileSystem.java:1194)
	at org.apache.hadoop.hdfs.DistributedFileSystem$25.doCall(DistributedFileSystem.java:1139)
	at org.apache.hadoop.hdfs.DistributedFileSystem$25.doCall(DistributedFileSystem.java:1135)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.listLocatedStatus(DistributedFileSystem.java:1153)
	at org.apache.hadoop.fs.FileSystem.listLocatedStatus(FileSystem.java:2054)
	at org.apache.hadoop.fs.FileSystem$5.<init>(FileSystem.java:2183)
	at org.apache.hadoop.fs.FileSystem.listFiles(FileSystem.java:2180)
	at it.unipi.hadoop.Driver.getOutputFiles(Driver.java:38)
	at it.unipi.hadoop.Driver.getRandomCentroids(Driver.java:61)
	at it.unipi.hadoop.Driver.main(Driver.java:210)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:318)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:232)
2020-06-30 18:50:10,641 INFO mapreduce.Job:  map 100% reduce 100%
2020-06-30 18:50:11,665 INFO mapreduce.Job: Job job_1593532013608_0178 completed successfully
2020-06-30 18:50:11,767 INFO mapreduce.Job:  map 100% reduce 100%
2020-06-30 18:50:11,861 INFO mapreduce.Job: Counters: 54
	File System Counters
		FILE: Number of bytes read=149
		FILE: Number of bytes written=436611
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=137569
		HDFS: Number of bytes written=135
		HDFS: Number of read operations=8
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Failed reduce tasks=1
		Launched map tasks=1
		Launched reduce tasks=2
		Rack-local map tasks=1
		Total time spent by all maps in occupied slots (ms)=8688
		Total time spent by all reduces in occupied slots (ms)=11218
		Total time spent by all map tasks (ms)=4344
		Total time spent by all reduce tasks (ms)=5609
		Total vcore-milliseconds taken by all map tasks=4344
		Total vcore-milliseconds taken by all reduce tasks=5609
		Total megabyte-milliseconds taken by all map tasks=1112064
		Total megabyte-milliseconds taken by all reduce tasks=1435904
	Map-Reduce Framework
		Map input records=1000
		Map output records=1000
		Map output bytes=142439
		Map output materialized bytes=149
		Input split bytes=130
		Combine input records=1000
		Combine output records=1
		Reduce input groups=1
		Reduce shuffle bytes=149
		Reduce input records=1
		Reduce output records=1
		Spilled Records=2
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=174
		CPU time spent (ms)=1140
		Physical memory (bytes) snapshot=439644160
		Virtual memory (bytes) snapshot=3839115264
		Total committed heap usage (bytes)=328204288
		Peak Map Physical memory (bytes)=266145792
		Peak Map Virtual memory (bytes)=1913151488
		Peak Reduce Physical memory (bytes)=173498368
		Peak Reduce Virtual memory (bytes)=1925963776
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=137439
	File Output Format Counters 
		Bytes Written=135
2020-06-30 18:50:11,960 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:50:12,015 INFO client.RMProxy: Connecting to ResourceManager at hadoop-namenode/172.16.1.142:8032
2020-06-30 18:50:12,047 WARN mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2020-06-30 18:50:12,062 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1593532013608_0191
2020-06-30 18:50:12,088 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:50:12,144 INFO input.FileInputFormat: Total input files to process : 1
2020-06-30 18:50:12,185 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:50:12,311 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:50:12,325 INFO mapreduce.JobSubmitter: number of splits:1
2020-06-30 18:50:12,364 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:50:12,400 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1593532013608_0191
2020-06-30 18:50:12,401 INFO mapreduce.JobSubmitter: Executing with tokens: []
2020-06-30 18:50:12,619 INFO impl.YarnClientImpl: Submitted application application_1593532013608_0191
2020-06-30 18:50:12,622 INFO mapreduce.Job: The url to track the job: http://hadoop-namenode:8088/proxy/application_1593532013608_0191/
2020-06-30 18:50:12,623 INFO mapreduce.Job: Running job: job_1593532013608_0191
2020-06-30 18:50:12,785 INFO mapreduce.Job: Job job_1593532013608_0181 completed successfully
2020-06-30 18:50:12,884 INFO mapreduce.Job: Counters: 54
	File System Counters
		FILE: Number of bytes read=74
		FILE: Number of bytes written=436429
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=59439
		HDFS: Number of bytes written=62
		HDFS: Number of read operations=8
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=3
	Job Counters 
		Failed reduce tasks=1
		Launched map tasks=1
		Launched reduce tasks=2
		Data-local map tasks=1
		Total time spent by all maps in occupied slots (ms)=4856
		Total time spent by all reduces in occupied slots (ms)=10546
		Total time spent by all map tasks (ms)=2428
		Total time spent by all reduce tasks (ms)=5273
		Total vcore-milliseconds taken by all map tasks=2428
		Total vcore-milliseconds taken by all reduce tasks=5273
		Total megabyte-milliseconds taken by all map tasks=621568
		Total megabyte-milliseconds taken by all reduce tasks=1349888
	Map-Reduce Framework
		Map input records=1000
		Map output records=1000
		Map output bytes=63309
		Map output materialized bytes=74
		Input split bytes=130
		Combine input records=1000
		Combine output records=1
		Reduce input groups=1
		Reduce shuffle bytes=74
		Reduce input records=1
		Reduce output records=1
		Spilled Records=2
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=132
		CPU time spent (ms)=1080
		Physical memory (bytes) snapshot=447500288
		Virtual memory (bytes) snapshot=3839791104
		Total committed heap usage (bytes)=332922880
		Peak Map Physical memory (bytes)=276574208
		Peak Map Virtual memory (bytes)=1916637184
		Peak Reduce Physical memory (bytes)=170926080
		Peak Reduce Virtual memory (bytes)=1923153920
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=59309
	File Output Format Counters 
		Bytes Written=62
Exception in thread "main" java.io.FileNotFoundException: File hdfs://hadoop-namenode:9820/user/hadoop/KMeansTemporaryUniformSampling does not exist.
	at org.apache.hadoop.hdfs.DistributedFileSystem$DirListingIterator.<init>(DistributedFileSystem.java:1220)
	at org.apache.hadoop.hdfs.DistributedFileSystem$DirListingIterator.<init>(DistributedFileSystem.java:1194)
	at org.apache.hadoop.hdfs.DistributedFileSystem$25.doCall(DistributedFileSystem.java:1139)
	at org.apache.hadoop.hdfs.DistributedFileSystem$25.doCall(DistributedFileSystem.java:1135)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.listLocatedStatus(DistributedFileSystem.java:1153)
	at org.apache.hadoop.fs.FileSystem.listLocatedStatus(FileSystem.java:2054)
	at org.apache.hadoop.fs.FileSystem$5.<init>(FileSystem.java:2183)
	at org.apache.hadoop.fs.FileSystem.listFiles(FileSystem.java:2180)
	at it.unipi.hadoop.Driver.getOutputFiles(Driver.java:38)
	at it.unipi.hadoop.Driver.getRandomCentroids(Driver.java:61)
	at it.unipi.hadoop.Driver.main(Driver.java:210)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:318)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:232)
2020-06-30 18:50:13,270 INFO mapreduce.Job: Job job_1593532013608_0187 running in uber mode : false
2020-06-30 18:50:13,271 INFO mapreduce.Job:  map 0% reduce 0%
2020-06-30 18:50:16,709 INFO mapreduce.Job: Job job_1593532013608_0189 running in uber mode : false
2020-06-30 18:50:16,710 INFO mapreduce.Job:  map 0% reduce 0%
2020-06-30 18:50:17,650 INFO mapreduce.Job: Job job_1593532013608_0188 running in uber mode : false
2020-06-30 18:50:17,650 INFO mapreduce.Job:  map 0% reduce 0%
2020-06-30 18:50:18,321 INFO mapreduce.Job:  map 100% reduce 0%
2020-06-30 18:50:20,749 INFO mapreduce.Job:  map 100% reduce 0%
2020-06-30 18:50:22,709 INFO mapreduce.Job: Job job_1593532013608_0191 running in uber mode : false
2020-06-30 18:50:22,709 INFO mapreduce.Job:  map 0% reduce 0%
2020-06-30 18:50:22,714 INFO mapreduce.Job:  map 100% reduce 0%
2020-06-30 18:50:24,353 INFO mapreduce.Job:  map 100% reduce 100%
2020-06-30 18:50:24,362 INFO mapreduce.Job: Job job_1593532013608_0187 completed successfully
2020-06-30 18:50:24,529 INFO mapreduce.Job: Counters: 53
	File System Counters
		FILE: Number of bytes read=152
		FILE: Number of bytes written=436621
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=13738638
		HDFS: Number of bytes written=138
		HDFS: Number of read operations=8
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=1
		Launched reduce tasks=1
		Rack-local map tasks=1
		Total time spent by all maps in occupied slots (ms)=5134
		Total time spent by all reduces in occupied slots (ms)=7308
		Total time spent by all map tasks (ms)=2567
		Total time spent by all reduce tasks (ms)=3654
		Total vcore-milliseconds taken by all map tasks=2567
		Total vcore-milliseconds taken by all reduce tasks=3654
		Total megabyte-milliseconds taken by all map tasks=657152
		Total megabyte-milliseconds taken by all reduce tasks=935424
	Map-Reduce Framework
		Map input records=100000
		Map output records=100000
		Map output bytes=14238502
		Map output materialized bytes=152
		Input split bytes=132
		Combine input records=100000
		Combine output records=1
		Reduce input groups=1
		Reduce shuffle bytes=152
		Reduce input records=1
		Reduce output records=1
		Spilled Records=2
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=201
		CPU time spent (ms)=1580
		Physical memory (bytes) snapshot=446550016
		Virtual memory (bytes) snapshot=3840778240
		Total committed heap usage (bytes)=333447168
		Peak Map Physical memory (bytes)=277176320
		Peak Map Virtual memory (bytes)=1917931520
		Peak Reduce Physical memory (bytes)=169373696
		Peak Reduce Virtual memory (bytes)=1922846720
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=13738506
	File Output Format Counters 
		Bytes Written=138
2020-06-30 18:50:24,591 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
java.lang.NumberFormatException: multiple points
	at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:1914)
	at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110)
	at java.lang.Double.parseDouble(Double.java:538)
	at it.unipi.hadoop.Point.parseString(Point.java:51)
	at it.unipi.hadoop.Driver.getRandomCentroids(Driver.java:67)
	at it.unipi.hadoop.Driver.main(Driver.java:210)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:318)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:232)
2020-06-30 18:50:24,684 INFO client.RMProxy: Connecting to ResourceManager at hadoop-namenode/172.16.1.142:8032
2020-06-30 18:50:24,702 WARN mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2020-06-30 18:50:24,714 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1593532013608_0192
2020-06-30 18:50:24,739 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:50:24,798 INFO input.FileInputFormat: Total input files to process : 1
2020-06-30 18:50:24,833 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:50:24,922 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:50:24,957 INFO mapreduce.JobSubmitter: number of splits:1
2020-06-30 18:50:25,015 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:50:25,067 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1593532013608_0192
2020-06-30 18:50:25,067 INFO mapreduce.JobSubmitter: Executing with tokens: []
2020-06-30 18:50:25,088 INFO impl.YarnClientImpl: Submitted application application_1593532013608_0192
2020-06-30 18:50:25,091 INFO mapreduce.Job: The url to track the job: http://hadoop-namenode:8088/proxy/application_1593532013608_0192/
2020-06-30 18:50:25,091 INFO mapreduce.Job: Running job: job_1593532013608_0192
2020-06-30 18:50:26,789 INFO mapreduce.Job:  map 100% reduce 100%
2020-06-30 18:50:26,806 INFO mapreduce.Job: Job job_1593532013608_0189 completed successfully
2020-06-30 18:50:26,847 INFO mapreduce.Job: Counters: 53
	File System Counters
		FILE: Number of bytes read=71
		FILE: Number of bytes written=436457
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=592955
		HDFS: Number of bytes written=59
		HDFS: Number of read operations=8
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=1
		Launched reduce tasks=1
		Data-local map tasks=1
		Total time spent by all maps in occupied slots (ms)=4642
		Total time spent by all reduces in occupied slots (ms)=6560
		Total time spent by all map tasks (ms)=2321
		Total time spent by all reduce tasks (ms)=3280
		Total vcore-milliseconds taken by all map tasks=2321
		Total vcore-milliseconds taken by all reduce tasks=3280
		Total megabyte-milliseconds taken by all map tasks=594176
		Total megabyte-milliseconds taken by all reduce tasks=839680
	Map-Reduce Framework
		Map input records=10000
		Map output records=10000
		Map output bytes=632824
		Map output materialized bytes=71
		Input split bytes=131
		Combine input records=10000
		Combine output records=1
		Reduce input groups=1
		Reduce shuffle bytes=71
		Reduce input records=1
		Reduce output records=1
		Spilled Records=2
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=214
		CPU time spent (ms)=1220
		Physical memory (bytes) snapshot=448413696
		Virtual memory (bytes) snapshot=3840868352
		Total committed heap usage (bytes)=335544320
		Peak Map Physical memory (bytes)=278036480
		Peak Map Virtual memory (bytes)=1917124608
		Peak Reduce Physical memory (bytes)=170377216
		Peak Reduce Virtual memory (bytes)=1923743744
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=592824
	File Output Format Counters 
		Bytes Written=59
2020-06-30 18:50:26,877 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
java.lang.NumberFormatException: For input string: "0.9225438603105027-0.7450012305934934"
	at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043)
	at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110)
	at java.lang.Double.parseDouble(Double.java:538)
	at it.unipi.hadoop.Point.parseString(Point.java:51)
	at it.unipi.hadoop.Driver.getRandomCentroids(Driver.java:67)
	at it.unipi.hadoop.Driver.main(Driver.java:210)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:318)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:232)
2020-06-30 18:50:26,929 INFO client.RMProxy: Connecting to ResourceManager at hadoop-namenode/172.16.1.142:8032
2020-06-30 18:50:26,953 WARN mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2020-06-30 18:50:26,965 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1593532013608_0193
2020-06-30 18:50:26,993 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:50:27,045 INFO input.FileInputFormat: Total input files to process : 1
2020-06-30 18:50:27,082 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:50:27,138 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:50:27,177 INFO mapreduce.JobSubmitter: number of splits:1
2020-06-30 18:50:27,211 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:50:27,248 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1593532013608_0193
2020-06-30 18:50:27,248 INFO mapreduce.JobSubmitter: Executing with tokens: []
2020-06-30 18:50:27,266 INFO impl.YarnClientImpl: Submitted application application_1593532013608_0193
2020-06-30 18:50:27,268 INFO mapreduce.Job: The url to track the job: http://hadoop-namenode:8088/proxy/application_1593532013608_0193/
2020-06-30 18:50:27,269 INFO mapreduce.Job: Running job: job_1593532013608_0193
2020-06-30 18:50:27,759 INFO mapreduce.Job:  map 100% reduce 0%
2020-06-30 18:50:27,763 INFO mapreduce.Job:  map 100% reduce 100%
2020-06-30 18:50:28,784 INFO mapreduce.Job: Job job_1593532013608_0188 completed successfully
2020-06-30 18:50:28,818 INFO mapreduce.Job: Counters: 53
	File System Counters
		FILE: Number of bytes read=153
		FILE: Number of bytes written=436557
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=1373765
		HDFS: Number of bytes written=139
		HDFS: Number of read operations=8
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=3
	Job Counters 
		Launched map tasks=1
		Launched reduce tasks=1
		Data-local map tasks=1
		Total time spent by all maps in occupied slots (ms)=5060
		Total time spent by all reduces in occupied slots (ms)=6368
		Total time spent by all map tasks (ms)=2530
		Total time spent by all reduce tasks (ms)=3184
		Total vcore-milliseconds taken by all map tasks=2530
		Total vcore-milliseconds taken by all reduce tasks=3184
		Total megabyte-milliseconds taken by all map tasks=647680
		Total megabyte-milliseconds taken by all reduce tasks=815104
	Map-Reduce Framework
		Map input records=10000
		Map output records=10000
		Map output bytes=1423634
		Map output materialized bytes=153
		Input split bytes=131
		Combine input records=10000
		Combine output records=1
		Reduce input groups=1
		Reduce shuffle bytes=153
		Reduce input records=1
		Reduce output records=1
		Spilled Records=2
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=141
		CPU time spent (ms)=1210
		Physical memory (bytes) snapshot=444645376
		Virtual memory (bytes) snapshot=3840962560
		Total committed heap usage (bytes)=329777152
		Peak Map Physical memory (bytes)=272691200
		Peak Map Virtual memory (bytes)=1914474496
		Peak Reduce Physical memory (bytes)=171954176
		Peak Reduce Virtual memory (bytes)=1926488064
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=1373634
	File Output Format Counters 
		Bytes Written=139
2020-06-30 18:50:28,837 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
java.lang.NumberFormatException: multiple points
	at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:1914)
	at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110)
	at java.lang.Double.parseDouble(Double.java:538)
	at it.unipi.hadoop.Point.parseString(Point.java:51)
	at it.unipi.hadoop.Driver.getRandomCentroids(Driver.java:67)
	at it.unipi.hadoop.Driver.main(Driver.java:210)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:318)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:232)
2020-06-30 18:50:28,905 INFO client.RMProxy: Connecting to ResourceManager at hadoop-namenode/172.16.1.142:8032
2020-06-30 18:50:28,937 WARN mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2020-06-30 18:50:28,954 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1593532013608_0194
2020-06-30 18:50:28,991 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:50:29,048 INFO input.FileInputFormat: Total input files to process : 1
2020-06-30 18:50:29,087 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:50:29,164 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:50:29,190 INFO mapreduce.JobSubmitter: number of splits:1
2020-06-30 18:50:29,247 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:50:29,290 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1593532013608_0194
2020-06-30 18:50:29,290 INFO mapreduce.JobSubmitter: Executing with tokens: []
2020-06-30 18:50:29,321 INFO impl.YarnClientImpl: Submitted application application_1593532013608_0194
2020-06-30 18:50:29,325 INFO mapreduce.Job: The url to track the job: http://hadoop-namenode:8088/proxy/application_1593532013608_0194/
2020-06-30 18:50:29,325 INFO mapreduce.Job: Running job: job_1593532013608_0194
2020-06-30 18:50:33,222 INFO mapreduce.Job: Job job_1593532013608_0192 running in uber mode : false
2020-06-30 18:50:33,222 INFO mapreduce.Job:  map 0% reduce 0%
2020-06-30 18:50:34,805 INFO mapreduce.Job:  map 100% reduce 100%
2020-06-30 18:50:34,817 INFO mapreduce.Job: Job job_1593532013608_0191 completed successfully
2020-06-30 18:50:34,853 INFO mapreduce.Job: Counters: 53
	File System Counters
		FILE: Number of bytes read=155
		FILE: Number of bytes written=436591
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=137569
		HDFS: Number of bytes written=141
		HDFS: Number of read operations=8
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=1
		Launched reduce tasks=1
		Rack-local map tasks=1
		Total time spent by all maps in occupied slots (ms)=6852
		Total time spent by all reduces in occupied slots (ms)=8446
		Total time spent by all map tasks (ms)=3426
		Total time spent by all reduce tasks (ms)=4223
		Total vcore-milliseconds taken by all map tasks=3426
		Total vcore-milliseconds taken by all reduce tasks=4223
		Total megabyte-milliseconds taken by all map tasks=877056
		Total megabyte-milliseconds taken by all reduce tasks=1081088
	Map-Reduce Framework
		Map input records=1000
		Map output records=1000
		Map output bytes=142439
		Map output materialized bytes=155
		Input split bytes=130
		Combine input records=1000
		Combine output records=1
		Reduce input groups=1
		Reduce shuffle bytes=155
		Reduce input records=1
		Reduce output records=1
		Spilled Records=2
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=146
		CPU time spent (ms)=1070
		Physical memory (bytes) snapshot=442724352
		Virtual memory (bytes) snapshot=3837214720
		Total committed heap usage (bytes)=319815680
		Peak Map Physical memory (bytes)=273944576
		Peak Map Virtual memory (bytes)=1913839616
		Peak Reduce Physical memory (bytes)=168779776
		Peak Reduce Virtual memory (bytes)=1923375104
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=137439
	File Output Format Counters 
		Bytes Written=141
2020-06-30 18:50:34,870 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
java.lang.NumberFormatException: multiple points
	at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:1914)
	at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110)
	at java.lang.Double.parseDouble(Double.java:538)
	at it.unipi.hadoop.Point.parseString(Point.java:51)
	at it.unipi.hadoop.Driver.getRandomCentroids(Driver.java:67)
	at it.unipi.hadoop.Driver.main(Driver.java:210)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:318)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:232)
2020-06-30 18:50:35,055 INFO client.RMProxy: Connecting to ResourceManager at hadoop-namenode/172.16.1.142:8032
2020-06-30 18:50:35,116 WARN mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2020-06-30 18:50:35,283 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1593532013608_0195
2020-06-30 18:50:35,383 INFO mapreduce.Job: Job job_1593532013608_0193 running in uber mode : false
2020-06-30 18:50:35,384 INFO mapreduce.Job:  map 0% reduce 0%
2020-06-30 18:50:35,901 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:50:36,380 INFO input.FileInputFormat: Total input files to process : 1
2020-06-30 18:50:36,691 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:50:37,049 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:50:37,105 INFO mapreduce.JobSubmitter: number of splits:1
2020-06-30 18:50:37,198 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:50:37,299 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1593532013608_0195
2020-06-30 18:50:37,299 INFO mapreduce.JobSubmitter: Executing with tokens: []
2020-06-30 18:50:37,581 INFO impl.YarnClientImpl: Submitted application application_1593532013608_0195
2020-06-30 18:50:37,584 INFO mapreduce.Job: The url to track the job: http://hadoop-namenode:8088/proxy/application_1593532013608_0195/
2020-06-30 18:50:37,584 INFO mapreduce.Job: Running job: job_1593532013608_0195
2020-06-30 18:50:43,702 INFO mapreduce.Job: Job job_1593532013608_0195 running in uber mode : false
2020-06-30 18:50:43,703 INFO mapreduce.Job:  map 0% reduce 0%
2020-06-30 18:50:44,517 INFO mapreduce.Job: Task Id : attempt_1593532013608_0192_m_000000_0, Status : FAILED
[2020-06-30 18:50:42.360]Container [pid=1346330,containerID=container_1593532013608_0192_01_000002] is running 4747264B beyond the 'PHYSICAL' memory limit. Current usage: 260.5 MB of 256 MB physical memory used; 1.8 GB of 1.3 GB virtual memory used. Killing container.
Dump of the process-tree for container_1593532013608_0192_01_000002 :
	|- PID PPID PGRPID SESSID CMD_NAME USER_MODE_TIME(MILLIS) SYSTEM_TIME(MILLIS) VMEM_USAGE(BYTES) RSSMEM_USAGE(PAGES) FULL_CMD_LINE
	|- 1346330 1346328 1346330 1346330 (bash) 0 0 13193216 800 /bin/bash -c /usr/lib/jvm/java-8-openjdk-amd64/bin/java -Djava.net.preferIPv4Stack=true -Dhadoop.metrics.log.level=WARN   -Xmx205m -Djava.io.tmpdir=/opt/yarn/local/usercache/hadoop/appcache/application_1593532013608_0192/container_1593532013608_0192_01_000002/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=/opt/yarn/logs/application_1593532013608_0192/container_1593532013608_0192_01_000002 -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Dhadoop.root.logfile=syslog org.apache.hadoop.mapred.YarnChild 172.16.1.142 43529 attempt_1593532013608_0192_m_000000_0 2 1>/opt/yarn/logs/application_1593532013608_0192/container_1593532013608_0192_01_000002/stdout 2>/opt/yarn/logs/application_1593532013608_0192/container_1593532013608_0192_01_000002/stderr  
	|- 1346342 1346330 1346330 1346330 (java) 357 26 1902485504 65895 /usr/lib/jvm/java-8-openjdk-amd64/bin/java -Djava.net.preferIPv4Stack=true -Dhadoop.metrics.log.level=WARN -Xmx205m -Djava.io.tmpdir=/opt/yarn/local/usercache/hadoop/appcache/application_1593532013608_0192/container_1593532013608_0192_01_000002/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=/opt/yarn/logs/application_1593532013608_0192/container_1593532013608_0192_01_000002 -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Dhadoop.root.logfile=syslog org.apache.hadoop.mapred.YarnChild 172.16.1.142 43529 attempt_1593532013608_0192_m_000000_0 2 

[2020-06-30 18:50:42.546]Container killed on request. Exit code is 143
[2020-06-30 18:50:42.554]Container exited with a non-zero exit code 143. 

2020-06-30 18:50:47,770 INFO mapreduce.Job:  map 100% reduce 0%
2020-06-30 18:50:48,625 INFO mapreduce.Job:  map 100% reduce 0%
2020-06-30 18:50:49,616 INFO mapreduce.Job: Job job_1593532013608_0194 running in uber mode : false
2020-06-30 18:50:49,616 INFO mapreduce.Job:  map 0% reduce 0%
2020-06-30 18:50:51,769 INFO mapreduce.Job:  map 100% reduce 0%
2020-06-30 18:50:52,808 INFO mapreduce.Job:  map 100% reduce 100%
2020-06-30 18:50:53,666 INFO mapreduce.Job:  map 100% reduce 100%
2020-06-30 18:50:53,829 INFO mapreduce.Job: Job job_1593532013608_0195 completed successfully
2020-06-30 18:50:53,861 INFO mapreduce.Job: Counters: 53
	File System Counters
		FILE: Number of bytes read=150
		FILE: Number of bytes written=436613
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=137569
		HDFS: Number of bytes written=136
		HDFS: Number of read operations=8
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=1
		Launched reduce tasks=1
		Rack-local map tasks=1
		Total time spent by all maps in occupied slots (ms)=4632
		Total time spent by all reduces in occupied slots (ms)=5406
		Total time spent by all map tasks (ms)=2316
		Total time spent by all reduce tasks (ms)=2703
		Total vcore-milliseconds taken by all map tasks=2316
		Total vcore-milliseconds taken by all reduce tasks=2703
		Total megabyte-milliseconds taken by all map tasks=592896
		Total megabyte-milliseconds taken by all reduce tasks=691968
	Map-Reduce Framework
		Map input records=1000
		Map output records=1000
		Map output bytes=142439
		Map output materialized bytes=150
		Input split bytes=130
		Combine input records=1000
		Combine output records=1
		Reduce input groups=1
		Reduce shuffle bytes=150
		Reduce input records=1
		Reduce output records=1
		Spilled Records=2
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=156
		CPU time spent (ms)=1040
		Physical memory (bytes) snapshot=446427136
		Virtual memory (bytes) snapshot=3838242816
		Total committed heap usage (bytes)=334495744
		Peak Map Physical memory (bytes)=274354176
		Peak Map Virtual memory (bytes)=1914945536
		Peak Reduce Physical memory (bytes)=172072960
		Peak Reduce Virtual memory (bytes)=1923297280
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=137439
	File Output Format Counters 
		Bytes Written=136
2020-06-30 18:50:53,866 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:50:53,887 INFO client.RMProxy: Connecting to ResourceManager at hadoop-namenode/172.16.1.142:8032
2020-06-30 18:50:53,895 WARN mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2020-06-30 18:50:53,907 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1593532013608_0196
2020-06-30 18:50:53,926 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:50:53,961 INFO input.FileInputFormat: Total input files to process : 1
2020-06-30 18:50:53,993 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:50:54,051 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:50:54,064 INFO mapreduce.JobSubmitter: number of splits:1
2020-06-30 18:50:54,112 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:50:54,135 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1593532013608_0196
2020-06-30 18:50:54,136 INFO mapreduce.JobSubmitter: Executing with tokens: []
2020-06-30 18:50:54,157 INFO impl.YarnClientImpl: Submitted application application_1593532013608_0196
2020-06-30 18:50:54,161 INFO mapreduce.Job: The url to track the job: http://hadoop-namenode:8088/proxy/application_1593532013608_0196/
2020-06-30 18:50:54,162 INFO mapreduce.Job: Running job: job_1593532013608_0196
2020-06-30 18:50:54,665 INFO mapreduce.Job:  map 100% reduce 0%
2020-06-30 18:50:54,680 INFO mapreduce.Job: Job job_1593532013608_0193 completed successfully
2020-06-30 18:50:54,720 INFO mapreduce.Job: Counters: 53
	File System Counters
		FILE: Number of bytes read=69
		FILE: Number of bytes written=436389
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=592955
		HDFS: Number of bytes written=57
		HDFS: Number of read operations=8
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=3
	Job Counters 
		Launched map tasks=1
		Launched reduce tasks=1
		Data-local map tasks=1
		Total time spent by all maps in occupied slots (ms)=23100
		Total time spent by all reduces in occupied slots (ms)=6180
		Total time spent by all map tasks (ms)=11550
		Total time spent by all reduce tasks (ms)=3090
		Total vcore-milliseconds taken by all map tasks=11550
		Total vcore-milliseconds taken by all reduce tasks=3090
		Total megabyte-milliseconds taken by all map tasks=2956800
		Total megabyte-milliseconds taken by all reduce tasks=791040
	Map-Reduce Framework
		Map input records=10000
		Map output records=10000
		Map output bytes=632824
		Map output materialized bytes=69
		Input split bytes=131
		Combine input records=10000
		Combine output records=1
		Reduce input groups=1
		Reduce shuffle bytes=69
		Reduce input records=1
		Reduce output records=1
		Spilled Records=2
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=306
		CPU time spent (ms)=1270
		Physical memory (bytes) snapshot=443199488
		Virtual memory (bytes) snapshot=3840282624
		Total committed heap usage (bytes)=322961408
		Peak Map Physical memory (bytes)=274391040
		Peak Map Virtual memory (bytes)=1917480960
		Peak Reduce Physical memory (bytes)=168808448
		Peak Reduce Virtual memory (bytes)=1922801664
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=592824
	File Output Format Counters 
		Bytes Written=57
Exception in thread "main" java.io.FileNotFoundException: File hdfs://hadoop-namenode:9820/user/hadoop/KMeansTemporaryUniformSampling does not exist.
	at org.apache.hadoop.hdfs.DistributedFileSystem$DirListingIterator.<init>(DistributedFileSystem.java:1220)
	at org.apache.hadoop.hdfs.DistributedFileSystem$DirListingIterator.<init>(DistributedFileSystem.java:1194)
	at org.apache.hadoop.hdfs.DistributedFileSystem$25.doCall(DistributedFileSystem.java:1139)
	at org.apache.hadoop.hdfs.DistributedFileSystem$25.doCall(DistributedFileSystem.java:1135)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.listLocatedStatus(DistributedFileSystem.java:1153)
	at org.apache.hadoop.fs.FileSystem.listLocatedStatus(FileSystem.java:2054)
	at org.apache.hadoop.fs.FileSystem$5.<init>(FileSystem.java:2183)
	at org.apache.hadoop.fs.FileSystem.listFiles(FileSystem.java:2180)
	at it.unipi.hadoop.Driver.getOutputFiles(Driver.java:38)
	at it.unipi.hadoop.Driver.getRandomCentroids(Driver.java:61)
	at it.unipi.hadoop.Driver.main(Driver.java:210)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:318)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:232)
2020-06-30 18:50:56,791 INFO mapreduce.Job:  map 100% reduce 100%
2020-06-30 18:50:57,803 INFO mapreduce.Job: Job job_1593532013608_0192 completed successfully
2020-06-30 18:50:57,839 INFO mapreduce.Job: Counters: 55
	File System Counters
		FILE: Number of bytes read=149
		FILE: Number of bytes written=436583
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=13738638
		HDFS: Number of bytes written=135
		HDFS: Number of read operations=8
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Failed map tasks=1
		Launched map tasks=2
		Launched reduce tasks=1
		Other local map tasks=1
		Data-local map tasks=1
		Total time spent by all maps in occupied slots (ms)=29626
		Total time spent by all reduces in occupied slots (ms)=5674
		Total time spent by all map tasks (ms)=14813
		Total time spent by all reduce tasks (ms)=2837
		Total vcore-milliseconds taken by all map tasks=14813
		Total vcore-milliseconds taken by all reduce tasks=2837
		Total megabyte-milliseconds taken by all map tasks=3792128
		Total megabyte-milliseconds taken by all reduce tasks=726272
	Map-Reduce Framework
		Map input records=100000
		Map output records=100000
		Map output bytes=14238502
		Map output materialized bytes=149
		Input split bytes=132
		Combine input records=100000
		Combine output records=1
		Reduce input groups=1
		Reduce shuffle bytes=149
		Reduce input records=1
		Reduce output records=1
		Spilled Records=2
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=149
		CPU time spent (ms)=1820
		Physical memory (bytes) snapshot=439910400
		Virtual memory (bytes) snapshot=3837976576
		Total committed heap usage (bytes)=319815680
		Peak Map Physical memory (bytes)=276258816
		Peak Map Virtual memory (bytes)=1916166144
		Peak Reduce Physical memory (bytes)=163651584
		Peak Reduce Virtual memory (bytes)=1921810432
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=13738506
	File Output Format Counters 
		Bytes Written=135
2020-06-30 18:50:57,849 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
java.lang.NumberFormatException: multiple points
	at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:1914)
	at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110)
	at java.lang.Double.parseDouble(Double.java:538)
	at it.unipi.hadoop.Point.parseString(Point.java:51)
	at it.unipi.hadoop.Driver.getRandomCentroids(Driver.java:67)
	at it.unipi.hadoop.Driver.main(Driver.java:210)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:318)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:232)
2020-06-30 18:50:57,873 INFO client.RMProxy: Connecting to ResourceManager at hadoop-namenode/172.16.1.142:8032
2020-06-30 18:50:57,887 WARN mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2020-06-30 18:50:57,896 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1593532013608_0197
2020-06-30 18:50:57,913 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:50:57,933 INFO input.FileInputFormat: Total input files to process : 1
2020-06-30 18:50:57,972 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:50:58,017 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:50:58,045 INFO mapreduce.JobSubmitter: number of splits:1
2020-06-30 18:50:58,078 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:50:58,103 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1593532013608_0197
2020-06-30 18:50:58,103 INFO mapreduce.JobSubmitter: Executing with tokens: []
2020-06-30 18:50:58,119 INFO impl.YarnClientImpl: Submitted application application_1593532013608_0197
2020-06-30 18:50:58,122 INFO mapreduce.Job: The url to track the job: http://hadoop-namenode:8088/proxy/application_1593532013608_0197/
2020-06-30 18:50:58,123 INFO mapreduce.Job: Running job: job_1593532013608_0197
2020-06-30 18:50:58,687 INFO mapreduce.Job: Task Id : attempt_1593532013608_0194_r_000000_0, Status : FAILED
Error: org.apache.hadoop.ipc.RemoteException(java.io.FileNotFoundException): File does not exist: /user/hadoop/KMeansTemporaryUniformSampling/_temporary/1/_temporary/attempt_1593532013608_0194_r_000000_0/part-r-00000 (inode 35525) Holder DFSClient_attempt_1593532013608_0194_r_000000_0_-1254252110_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2840)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.completeFileInternal(FSDirWriteFileOp.java:693)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.completeFile(FSDirWriteFileOp.java:679)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:2883)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:947)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:614)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:527)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1036)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1000)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:928)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2916)

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1545)
	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
	at com.sun.proxy.$Proxy13.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:554)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy14.complete(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:949)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:907)
	at org.apache.hadoop.hdfs.DFSOutputStream.closeImpl(DFSOutputStream.java:890)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:845)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:101)
	at org.apache.hadoop.mapreduce.lib.output.TextOutputFormat$LineRecordWriter.close(TextOutputFormat.java:106)
	at org.apache.hadoop.mapred.ReduceTask$NewTrackingRecordWriter.close(ReduceTask.java:551)
	at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:630)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:390)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)

2020-06-30 18:51:03,262 INFO mapreduce.Job: Job job_1593532013608_0196 running in uber mode : false
2020-06-30 18:51:03,262 INFO mapreduce.Job:  map 0% reduce 0%
2020-06-30 18:51:04,187 INFO mapreduce.Job: Job job_1593532013608_0197 running in uber mode : false
2020-06-30 18:51:04,187 INFO mapreduce.Job:  map 0% reduce 0%
2020-06-30 18:51:04,727 INFO mapreduce.Job:  map 100% reduce 100%
2020-06-30 18:51:04,734 INFO mapreduce.Job: Job job_1593532013608_0194 completed successfully
2020-06-30 18:51:04,763 INFO mapreduce.Job: Counters: 54
	File System Counters
		FILE: Number of bytes read=152
		FILE: Number of bytes written=436619
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=1373765
		HDFS: Number of bytes written=138
		HDFS: Number of read operations=8
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Failed reduce tasks=1
		Launched map tasks=1
		Launched reduce tasks=2
		Data-local map tasks=1
		Total time spent by all maps in occupied slots (ms)=6594
		Total time spent by all reduces in occupied slots (ms)=9076
		Total time spent by all map tasks (ms)=3297
		Total time spent by all reduce tasks (ms)=4538
		Total vcore-milliseconds taken by all map tasks=3297
		Total vcore-milliseconds taken by all reduce tasks=4538
		Total megabyte-milliseconds taken by all map tasks=844032
		Total megabyte-milliseconds taken by all reduce tasks=1161728
	Map-Reduce Framework
		Map input records=10000
		Map output records=10000
		Map output bytes=1423634
		Map output materialized bytes=152
		Input split bytes=131
		Combine input records=10000
		Combine output records=1
		Reduce input groups=1
		Reduce shuffle bytes=152
		Reduce input records=1
		Reduce output records=1
		Spilled Records=2
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=165
		CPU time spent (ms)=1120
		Physical memory (bytes) snapshot=439304192
		Virtual memory (bytes) snapshot=3837796352
		Total committed heap usage (bytes)=330301440
		Peak Map Physical memory (bytes)=267833344
		Peak Map Virtual memory (bytes)=1914605568
		Peak Reduce Physical memory (bytes)=171470848
		Peak Reduce Virtual memory (bytes)=1923190784
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=1373634
	File Output Format Counters 
		Bytes Written=138
2020-06-30 18:51:04,776 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
java.lang.NumberFormatException: multiple points
	at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:1914)
	at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110)
	at java.lang.Double.parseDouble(Double.java:538)
	at it.unipi.hadoop.Point.parseString(Point.java:51)
	at it.unipi.hadoop.Driver.getRandomCentroids(Driver.java:67)
	at it.unipi.hadoop.Driver.main(Driver.java:210)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:318)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:232)
2020-06-30 18:51:04,797 INFO client.RMProxy: Connecting to ResourceManager at hadoop-namenode/172.16.1.142:8032
2020-06-30 18:51:04,806 WARN mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2020-06-30 18:51:04,821 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1593532013608_0198
2020-06-30 18:51:04,840 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:51:04,865 INFO input.FileInputFormat: Total input files to process : 1
2020-06-30 18:51:04,895 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:51:04,955 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:51:04,972 INFO mapreduce.JobSubmitter: number of splits:1
2020-06-30 18:51:05,001 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:51:05,026 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1593532013608_0198
2020-06-30 18:51:05,026 INFO mapreduce.JobSubmitter: Executing with tokens: []
2020-06-30 18:51:05,039 INFO impl.YarnClientImpl: Submitted application application_1593532013608_0198
2020-06-30 18:51:05,041 INFO mapreduce.Job: The url to track the job: http://hadoop-namenode:8088/proxy/application_1593532013608_0198/
2020-06-30 18:51:05,041 INFO mapreduce.Job: Running job: job_1593532013608_0198
2020-06-30 18:51:08,303 INFO mapreduce.Job:  map 100% reduce 0%
2020-06-30 18:51:09,227 INFO mapreduce.Job:  map 100% reduce 0%
2020-06-30 18:51:11,109 INFO mapreduce.Job: Job job_1593532013608_0198 running in uber mode : false
2020-06-30 18:51:11,109 INFO mapreduce.Job:  map 0% reduce 0%
2020-06-30 18:51:12,336 INFO mapreduce.Job:  map 100% reduce 100%
2020-06-30 18:51:13,370 INFO mapreduce.Job: Job job_1593532013608_0196 completed successfully
2020-06-30 18:51:13,534 INFO mapreduce.Job: Counters: 53
	File System Counters
		FILE: Number of bytes read=157
		FILE: Number of bytes written=436627
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=137569
		HDFS: Number of bytes written=143
		HDFS: Number of read operations=8
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=1
		Launched reduce tasks=1
		Rack-local map tasks=1
		Total time spent by all maps in occupied slots (ms)=4668
		Total time spent by all reduces in occupied slots (ms)=4718
		Total time spent by all map tasks (ms)=2334
		Total time spent by all reduce tasks (ms)=2359
		Total vcore-milliseconds taken by all map tasks=2334
		Total vcore-milliseconds taken by all reduce tasks=2359
		Total megabyte-milliseconds taken by all map tasks=597504
		Total megabyte-milliseconds taken by all reduce tasks=603904
	Map-Reduce Framework
		Map input records=1000
		Map output records=1000
		Map output bytes=142439
		Map output materialized bytes=157
		Input split bytes=130
		Combine input records=1000
		Combine output records=1
		Reduce input groups=1
		Reduce shuffle bytes=157
		Reduce input records=1
		Reduce output records=1
		Spilled Records=2
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=138
		CPU time spent (ms)=1000
		Physical memory (bytes) snapshot=444416000
		Virtual memory (bytes) snapshot=3838435328
		Total committed heap usage (bytes)=331874304
		Peak Map Physical memory (bytes)=274333696
		Peak Map Virtual memory (bytes)=1915006976
		Peak Reduce Physical memory (bytes)=170082304
		Peak Reduce Virtual memory (bytes)=1923428352
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=137439
	File Output Format Counters 
		Bytes Written=143
2020-06-30 18:51:13,556 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
java.lang.NumberFormatException: For input string: "-0.20064027611067492-0.6715645018674057"
	at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043)
	at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110)
	at java.lang.Double.parseDouble(Double.java:538)
	at it.unipi.hadoop.Point.parseString(Point.java:51)
	at it.unipi.hadoop.Driver.getRandomCentroids(Driver.java:67)
	at it.unipi.hadoop.Driver.main(Driver.java:210)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:318)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:232)
2020-06-30 18:51:13,580 INFO client.RMProxy: Connecting to ResourceManager at hadoop-namenode/172.16.1.142:8032
2020-06-30 18:51:13,591 WARN mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2020-06-30 18:51:13,603 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1593532013608_0199
2020-06-30 18:51:13,631 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:51:13,695 INFO input.FileInputFormat: Total input files to process : 1
2020-06-30 18:51:13,715 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:51:13,780 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:51:13,793 INFO mapreduce.JobSubmitter: number of splits:1
2020-06-30 18:51:13,820 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:51:13,860 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1593532013608_0199
2020-06-30 18:51:13,860 INFO mapreduce.JobSubmitter: Executing with tokens: []
2020-06-30 18:51:13,880 INFO impl.YarnClientImpl: Submitted application application_1593532013608_0199
2020-06-30 18:51:13,885 INFO mapreduce.Job: The url to track the job: http://hadoop-namenode:8088/proxy/application_1593532013608_0199/
2020-06-30 18:51:13,886 INFO mapreduce.Job: Running job: job_1593532013608_0199
2020-06-30 18:51:16,278 INFO mapreduce.Job:  map 100% reduce 100%
2020-06-30 18:51:16,297 INFO mapreduce.Job: Job job_1593532013608_0197 completed successfully
2020-06-30 18:51:16,350 INFO mapreduce.Job: Counters: 53
	File System Counters
		FILE: Number of bytes read=151
		FILE: Number of bytes written=436619
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=13738638
		HDFS: Number of bytes written=137
		HDFS: Number of read operations=8
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=1
		Launched reduce tasks=1
		Rack-local map tasks=1
		Total time spent by all maps in occupied slots (ms)=5132
		Total time spent by all reduces in occupied slots (ms)=10020
		Total time spent by all map tasks (ms)=2566
		Total time spent by all reduce tasks (ms)=5010
		Total vcore-milliseconds taken by all map tasks=2566
		Total vcore-milliseconds taken by all reduce tasks=5010
		Total megabyte-milliseconds taken by all map tasks=656896
		Total megabyte-milliseconds taken by all reduce tasks=1282560
	Map-Reduce Framework
		Map input records=100000
		Map output records=100000
		Map output bytes=14238502
		Map output materialized bytes=151
		Input split bytes=132
		Combine input records=100000
		Combine output records=1
		Reduce input groups=1
		Reduce shuffle bytes=151
		Reduce input records=1
		Reduce output records=1
		Spilled Records=2
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=217
		CPU time spent (ms)=1810
		Physical memory (bytes) snapshot=446955520
		Virtual memory (bytes) snapshot=3840360448
		Total committed heap usage (bytes)=335544320
		Peak Map Physical memory (bytes)=277180416
		Peak Map Virtual memory (bytes)=1917706240
		Peak Reduce Physical memory (bytes)=169775104
		Peak Reduce Virtual memory (bytes)=1922654208
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=13738506
	File Output Format Counters 
		Bytes Written=137
2020-06-30 18:51:16,356 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
java.lang.NumberFormatException: multiple points
	at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:1914)
	at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110)
	at java.lang.Double.parseDouble(Double.java:538)
	at it.unipi.hadoop.Point.parseString(Point.java:51)
	at it.unipi.hadoop.Driver.getRandomCentroids(Driver.java:67)
	at it.unipi.hadoop.Driver.main(Driver.java:210)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:318)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:232)
2020-06-30 18:51:16,378 INFO client.RMProxy: Connecting to ResourceManager at hadoop-namenode/172.16.1.142:8032
2020-06-30 18:51:16,386 WARN mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2020-06-30 18:51:16,397 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1593532013608_0200
2020-06-30 18:51:16,417 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:51:16,443 INFO input.FileInputFormat: Total input files to process : 1
2020-06-30 18:51:16,466 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:51:16,546 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:51:16,562 INFO mapreduce.JobSubmitter: number of splits:1
2020-06-30 18:51:16,596 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:51:16,639 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1593532013608_0200
2020-06-30 18:51:16,639 INFO mapreduce.JobSubmitter: Executing with tokens: []
2020-06-30 18:51:16,654 INFO impl.YarnClientImpl: Submitted application application_1593532013608_0200
2020-06-30 18:51:16,663 INFO mapreduce.Job: The url to track the job: http://hadoop-namenode:8088/proxy/application_1593532013608_0200/
2020-06-30 18:51:16,666 INFO mapreduce.Job: Running job: job_1593532013608_0200
2020-06-30 18:51:18,146 INFO mapreduce.Job:  map 100% reduce 0%
2020-06-30 18:51:19,951 INFO mapreduce.Job: Job job_1593532013608_0199 running in uber mode : false
2020-06-30 18:51:19,951 INFO mapreduce.Job:  map 0% reduce 0%
2020-06-30 18:51:22,742 INFO mapreduce.Job: Job job_1593532013608_0200 running in uber mode : false
2020-06-30 18:51:22,742 INFO mapreduce.Job:  map 0% reduce 0%
2020-06-30 18:51:23,176 INFO mapreduce.Job:  map 100% reduce 100%
2020-06-30 18:51:24,189 INFO mapreduce.Job: Job job_1593532013608_0198 completed successfully
2020-06-30 18:51:24,216 INFO mapreduce.Job: Counters: 53
	File System Counters
		FILE: Number of bytes read=155
		FILE: Number of bytes written=436625
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=1373765
		HDFS: Number of bytes written=141
		HDFS: Number of read operations=8
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=1
		Launched reduce tasks=1
		Rack-local map tasks=1
		Total time spent by all maps in occupied slots (ms)=10394
		Total time spent by all reduces in occupied slots (ms)=5046
		Total time spent by all map tasks (ms)=5197
		Total time spent by all reduce tasks (ms)=2523
		Total vcore-milliseconds taken by all map tasks=5197
		Total vcore-milliseconds taken by all reduce tasks=2523
		Total megabyte-milliseconds taken by all map tasks=1330432
		Total megabyte-milliseconds taken by all reduce tasks=645888
	Map-Reduce Framework
		Map input records=10000
		Map output records=10000
		Map output bytes=1423634
		Map output materialized bytes=155
		Input split bytes=131
		Combine input records=10000
		Combine output records=1
		Reduce input groups=1
		Reduce shuffle bytes=155
		Reduce input records=1
		Reduce output records=1
		Spilled Records=2
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=168
		CPU time spent (ms)=1220
		Physical memory (bytes) snapshot=441556992
		Virtual memory (bytes) snapshot=3838996480
		Total committed heap usage (bytes)=325058560
		Peak Map Physical memory (bytes)=272154624
		Peak Map Virtual memory (bytes)=1916899328
		Peak Reduce Physical memory (bytes)=169402368
		Peak Reduce Virtual memory (bytes)=1922097152
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=1373634
	File Output Format Counters 
		Bytes Written=141
2020-06-30 18:51:24,229 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
java.lang.NumberFormatException: For input string: "-0.42432794624357895-0.20943157124474276"
	at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043)
	at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110)
	at java.lang.Double.parseDouble(Double.java:538)
	at it.unipi.hadoop.Point.parseString(Point.java:51)
	at it.unipi.hadoop.Driver.getRandomCentroids(Driver.java:67)
	at it.unipi.hadoop.Driver.main(Driver.java:210)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:318)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:232)
2020-06-30 18:51:24,262 INFO client.RMProxy: Connecting to ResourceManager at hadoop-namenode/172.16.1.142:8032
2020-06-30 18:51:24,271 WARN mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2020-06-30 18:51:24,283 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1593532013608_0201
2020-06-30 18:51:24,317 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:51:24,356 INFO input.FileInputFormat: Total input files to process : 1
2020-06-30 18:51:24,379 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:51:24,427 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:51:24,456 INFO mapreduce.JobSubmitter: number of splits:1
2020-06-30 18:51:24,484 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:51:24,507 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1593532013608_0201
2020-06-30 18:51:24,507 INFO mapreduce.JobSubmitter: Executing with tokens: []
2020-06-30 18:51:24,527 INFO impl.YarnClientImpl: Submitted application application_1593532013608_0201
2020-06-30 18:51:24,529 INFO mapreduce.Job: The url to track the job: http://hadoop-namenode:8088/proxy/application_1593532013608_0201/
2020-06-30 18:51:24,529 INFO mapreduce.Job: Running job: job_1593532013608_0201
2020-06-30 18:51:25,000 INFO mapreduce.Job:  map 100% reduce 0%
2020-06-30 18:51:26,777 INFO mapreduce.Job:  map 100% reduce 0%
2020-06-30 18:51:30,028 INFO mapreduce.Job:  map 100% reduce 100%
2020-06-30 18:51:30,035 INFO mapreduce.Job: Job job_1593532013608_0199 completed successfully
2020-06-30 18:51:30,079 INFO mapreduce.Job: Counters: 53
	File System Counters
		FILE: Number of bytes read=152
		FILE: Number of bytes written=436553
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=137569
		HDFS: Number of bytes written=138
		HDFS: Number of read operations=8
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=1
		Launched reduce tasks=1
		Data-local map tasks=1
		Total time spent by all maps in occupied slots (ms)=4796
		Total time spent by all reduces in occupied slots (ms)=4668
		Total time spent by all map tasks (ms)=2398
		Total time spent by all reduce tasks (ms)=2334
		Total vcore-milliseconds taken by all map tasks=2398
		Total vcore-milliseconds taken by all reduce tasks=2334
		Total megabyte-milliseconds taken by all map tasks=613888
		Total megabyte-milliseconds taken by all reduce tasks=597504
	Map-Reduce Framework
		Map input records=1000
		Map output records=1000
		Map output bytes=142439
		Map output materialized bytes=152
		Input split bytes=130
		Combine input records=1000
		Combine output records=1
		Reduce input groups=1
		Reduce shuffle bytes=152
		Reduce input records=1
		Reduce output records=1
		Spilled Records=2
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=138
		CPU time spent (ms)=1090
		Physical memory (bytes) snapshot=446668800
		Virtual memory (bytes) snapshot=3839152128
		Total committed heap usage (bytes)=329252864
		Peak Map Physical memory (bytes)=276668416
		Peak Map Virtual memory (bytes)=1917173760
		Peak Reduce Physical memory (bytes)=170000384
		Peak Reduce Virtual memory (bytes)=1921978368
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=137439
	File Output Format Counters 
		Bytes Written=138
2020-06-30 18:51:30,091 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
java.lang.NumberFormatException: multiple points
	at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:1914)
	at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110)
	at java.lang.Double.parseDouble(Double.java:538)
	at it.unipi.hadoop.Point.parseString(Point.java:51)
	at it.unipi.hadoop.Driver.getRandomCentroids(Driver.java:67)
	at it.unipi.hadoop.Driver.main(Driver.java:210)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:318)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:232)
2020-06-30 18:51:30,117 INFO client.RMProxy: Connecting to ResourceManager at hadoop-namenode/172.16.1.142:8032
Exception in thread "main" org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory hdfs://hadoop-namenode:9820/user/hadoop/KMeansTemporaryUniformSampling already exists
	at org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:164)
	at org.apache.hadoop.mapreduce.JobSubmitter.checkSpecs(JobSubmitter.java:277)
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:143)
	at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1570)
	at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1567)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:1567)
	at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1588)
	at it.unipi.hadoop.Driver.getRandomCentroids(Driver.java:59)
	at it.unipi.hadoop.Driver.main(Driver.java:210)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:318)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:232)
2020-06-30 18:51:30,586 INFO mapreduce.Job: Job job_1593532013608_0201 running in uber mode : false
2020-06-30 18:51:30,586 INFO mapreduce.Job:  map 0% reduce 0%
2020-06-30 18:51:31,810 INFO mapreduce.Job:  map 100% reduce 100%
2020-06-30 18:51:31,818 INFO mapreduce.Job: Job job_1593532013608_0200 completed successfully
2020-06-30 18:51:31,851 INFO mapreduce.Job: Counters: 53
	File System Counters
		FILE: Number of bytes read=150
		FILE: Number of bytes written=436617
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=13738638
		HDFS: Number of bytes written=136
		HDFS: Number of read operations=8
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=1
		Launched reduce tasks=1
		Rack-local map tasks=1
		Total time spent by all maps in occupied slots (ms)=5132
		Total time spent by all reduces in occupied slots (ms)=4796
		Total time spent by all map tasks (ms)=2566
		Total time spent by all reduce tasks (ms)=2398
		Total vcore-milliseconds taken by all map tasks=2566
		Total vcore-milliseconds taken by all reduce tasks=2398
		Total megabyte-milliseconds taken by all map tasks=656896
		Total megabyte-milliseconds taken by all reduce tasks=613888
	Map-Reduce Framework
		Map input records=100000
		Map output records=100000
		Map output bytes=14238502
		Map output materialized bytes=150
		Input split bytes=132
		Combine input records=100000
		Combine output records=1
		Reduce input groups=1
		Reduce shuffle bytes=150
		Reduce input records=1
		Reduce output records=1
		Spilled Records=2
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=162
		CPU time spent (ms)=1630
		Physical memory (bytes) snapshot=446783488
		Virtual memory (bytes) snapshot=3841146880
		Total committed heap usage (bytes)=330301440
		Peak Map Physical memory (bytes)=277118976
		Peak Map Virtual memory (bytes)=1917349888
		Peak Reduce Physical memory (bytes)=169664512
		Peak Reduce Virtual memory (bytes)=1923796992
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=13738506
	File Output Format Counters 
		Bytes Written=136
2020-06-30 18:51:31,858 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
java.lang.NumberFormatException: multiple points
	at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:1914)
	at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110)
	at java.lang.Double.parseDouble(Double.java:538)
	at it.unipi.hadoop.Point.parseString(Point.java:51)
	at it.unipi.hadoop.Driver.getRandomCentroids(Driver.java:67)
	at it.unipi.hadoop.Driver.main(Driver.java:210)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:318)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:232)
2020-06-30 18:51:31,882 INFO client.RMProxy: Connecting to ResourceManager at hadoop-namenode/172.16.1.142:8032
2020-06-30 18:51:31,897 WARN mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2020-06-30 18:51:31,906 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1593532013608_0202
2020-06-30 18:51:31,927 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:51:31,954 INFO input.FileInputFormat: Total input files to process : 1
2020-06-30 18:51:31,979 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:51:32,017 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:51:32,047 INFO mapreduce.JobSubmitter: number of splits:1
2020-06-30 18:51:32,090 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:51:32,115 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1593532013608_0202
2020-06-30 18:51:32,115 INFO mapreduce.JobSubmitter: Executing with tokens: []
2020-06-30 18:51:32,132 INFO impl.YarnClientImpl: Submitted application application_1593532013608_0202
2020-06-30 18:51:32,136 INFO mapreduce.Job: The url to track the job: http://hadoop-namenode:8088/proxy/application_1593532013608_0202/
2020-06-30 18:51:32,136 INFO mapreduce.Job: Running job: job_1593532013608_0202
2020-06-30 18:51:34,616 INFO mapreduce.Job:  map 100% reduce 0%
2020-06-30 18:51:37,228 INFO mapreduce.Job: Job job_1593532013608_0202 running in uber mode : false
2020-06-30 18:51:37,228 INFO mapreduce.Job:  map 0% reduce 0%
2020-06-30 18:51:39,646 INFO mapreduce.Job:  map 100% reduce 100%
2020-06-30 18:51:39,657 INFO mapreduce.Job: Job job_1593532013608_0201 completed successfully
2020-06-30 18:51:39,692 INFO mapreduce.Job: Counters: 53
	File System Counters
		FILE: Number of bytes read=148
		FILE: Number of bytes written=436611
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=1373765
		HDFS: Number of bytes written=134
		HDFS: Number of read operations=8
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=1
		Launched reduce tasks=1
		Rack-local map tasks=1
		Total time spent by all maps in occupied slots (ms)=4434
		Total time spent by all reduces in occupied slots (ms)=4384
		Total time spent by all map tasks (ms)=2217
		Total time spent by all reduce tasks (ms)=2192
		Total vcore-milliseconds taken by all map tasks=2217
		Total vcore-milliseconds taken by all reduce tasks=2192
		Total megabyte-milliseconds taken by all map tasks=567552
		Total megabyte-milliseconds taken by all reduce tasks=561152
	Map-Reduce Framework
		Map input records=10000
		Map output records=10000
		Map output bytes=1423634
		Map output materialized bytes=148
		Input split bytes=131
		Combine input records=10000
		Combine output records=1
		Reduce input groups=1
		Reduce shuffle bytes=148
		Reduce input records=1
		Reduce output records=1
		Spilled Records=2
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=120
		CPU time spent (ms)=1250
		Physical memory (bytes) snapshot=447148032
		Virtual memory (bytes) snapshot=3840827392
		Total committed heap usage (bytes)=330301440
		Peak Map Physical memory (bytes)=273227776
		Peak Map Virtual memory (bytes)=1913659392
		Peak Reduce Physical memory (bytes)=173920256
		Peak Reduce Virtual memory (bytes)=1927168000
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=1373634
	File Output Format Counters 
		Bytes Written=134
2020-06-30 18:51:39,699 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
java.lang.NumberFormatException: multiple points
	at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:1914)
	at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110)
	at java.lang.Double.parseDouble(Double.java:538)
	at it.unipi.hadoop.Point.parseString(Point.java:51)
	at it.unipi.hadoop.Driver.getRandomCentroids(Driver.java:67)
	at it.unipi.hadoop.Driver.main(Driver.java:210)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:318)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:232)
2020-06-30 18:51:39,729 INFO client.RMProxy: Connecting to ResourceManager at hadoop-namenode/172.16.1.142:8032
2020-06-30 18:51:39,740 WARN mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2020-06-30 18:51:39,755 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1593532013608_0203
2020-06-30 18:51:39,781 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:51:39,817 INFO input.FileInputFormat: Total input files to process : 1
2020-06-30 18:51:39,844 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:51:39,912 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:51:39,963 INFO mapreduce.JobSubmitter: number of splits:1
2020-06-30 18:51:39,985 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:51:40,005 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1593532013608_0203
2020-06-30 18:51:40,005 INFO mapreduce.JobSubmitter: Executing with tokens: []
2020-06-30 18:51:40,018 INFO impl.YarnClientImpl: Submitted application application_1593532013608_0203
2020-06-30 18:51:40,024 INFO mapreduce.Job: The url to track the job: http://hadoop-namenode:8088/proxy/application_1593532013608_0203/
2020-06-30 18:51:40,024 INFO mapreduce.Job: Running job: job_1593532013608_0203
2020-06-30 18:51:42,259 INFO mapreduce.Job:  map 100% reduce 0%
2020-06-30 18:51:46,078 INFO mapreduce.Job: Job job_1593532013608_0203 running in uber mode : false
2020-06-30 18:51:46,078 INFO mapreduce.Job:  map 0% reduce 0%
2020-06-30 18:51:47,287 INFO mapreduce.Job:  map 100% reduce 100%
2020-06-30 18:51:47,297 INFO mapreduce.Job: Job job_1593532013608_0202 completed successfully
2020-06-30 18:51:47,358 INFO mapreduce.Job: Counters: 53
	File System Counters
		FILE: Number of bytes read=148
		FILE: Number of bytes written=436613
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=13738638
		HDFS: Number of bytes written=134
		HDFS: Number of read operations=8
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=1
		Launched reduce tasks=1
		Data-local map tasks=1
		Total time spent by all maps in occupied slots (ms)=4872
		Total time spent by all reduces in occupied slots (ms)=4380
		Total time spent by all map tasks (ms)=2436
		Total time spent by all reduce tasks (ms)=2190
		Total vcore-milliseconds taken by all map tasks=2436
		Total vcore-milliseconds taken by all reduce tasks=2190
		Total megabyte-milliseconds taken by all map tasks=623616
		Total megabyte-milliseconds taken by all reduce tasks=560640
	Map-Reduce Framework
		Map input records=100000
		Map output records=100000
		Map output bytes=14238502
		Map output materialized bytes=148
		Input split bytes=132
		Combine input records=100000
		Combine output records=1
		Reduce input groups=1
		Reduce shuffle bytes=148
		Reduce input records=1
		Reduce output records=1
		Spilled Records=2
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=121
		CPU time spent (ms)=1600
		Physical memory (bytes) snapshot=450445312
		Virtual memory (bytes) snapshot=3843764224
		Total committed heap usage (bytes)=333447168
		Peak Map Physical memory (bytes)=277413888
		Peak Map Virtual memory (bytes)=1917796352
		Peak Reduce Physical memory (bytes)=173031424
		Peak Reduce Virtual memory (bytes)=1925967872
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=13738506
	File Output Format Counters 
		Bytes Written=134
2020-06-30 18:51:47,364 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
java.lang.NumberFormatException: For input string: "0.28807041940697653-0.5706066574792754"
	at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043)
	at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110)
	at java.lang.Double.parseDouble(Double.java:538)
	at it.unipi.hadoop.Point.parseString(Point.java:51)
	at it.unipi.hadoop.Driver.getRandomCentroids(Driver.java:67)
	at it.unipi.hadoop.Driver.main(Driver.java:210)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:318)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:232)
2020-06-30 18:51:47,397 INFO client.RMProxy: Connecting to ResourceManager at hadoop-namenode/172.16.1.142:8032
2020-06-30 18:51:47,413 WARN mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2020-06-30 18:51:47,430 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1593532013608_0204
2020-06-30 18:51:47,446 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:51:47,483 INFO input.FileInputFormat: Total input files to process : 1
2020-06-30 18:51:47,508 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:51:47,559 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:51:47,572 INFO mapreduce.JobSubmitter: number of splits:1
2020-06-30 18:51:47,607 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:51:47,632 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1593532013608_0204
2020-06-30 18:51:47,632 INFO mapreduce.JobSubmitter: Executing with tokens: []
2020-06-30 18:51:47,850 INFO impl.YarnClientImpl: Submitted application application_1593532013608_0204
2020-06-30 18:51:47,852 INFO mapreduce.Job: The url to track the job: http://hadoop-namenode:8088/proxy/application_1593532013608_0204/
2020-06-30 18:51:47,852 INFO mapreduce.Job: Running job: job_1593532013608_0204
2020-06-30 18:51:50,113 INFO mapreduce.Job:  map 100% reduce 0%
2020-06-30 18:51:53,920 INFO mapreduce.Job: Job job_1593532013608_0204 running in uber mode : false
2020-06-30 18:51:53,921 INFO mapreduce.Job:  map 0% reduce 0%
2020-06-30 18:51:55,148 INFO mapreduce.Job:  map 100% reduce 100%
2020-06-30 18:51:55,161 INFO mapreduce.Job: Job job_1593532013608_0203 completed successfully
2020-06-30 18:51:55,197 INFO mapreduce.Job: Counters: 53
	File System Counters
		FILE: Number of bytes read=150
		FILE: Number of bytes written=436615
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=1373765
		HDFS: Number of bytes written=136
		HDFS: Number of read operations=8
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=1
		Launched reduce tasks=1
		Rack-local map tasks=1
		Total time spent by all maps in occupied slots (ms)=4632
		Total time spent by all reduces in occupied slots (ms)=4402
		Total time spent by all map tasks (ms)=2316
		Total time spent by all reduce tasks (ms)=2201
		Total vcore-milliseconds taken by all map tasks=2316
		Total vcore-milliseconds taken by all reduce tasks=2201
		Total megabyte-milliseconds taken by all map tasks=592896
		Total megabyte-milliseconds taken by all reduce tasks=563456
	Map-Reduce Framework
		Map input records=10000
		Map output records=10000
		Map output bytes=1423634
		Map output materialized bytes=150
		Input split bytes=131
		Combine input records=10000
		Combine output records=1
		Reduce input groups=1
		Reduce shuffle bytes=150
		Reduce input records=1
		Reduce output records=1
		Spilled Records=2
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=128
		CPU time spent (ms)=1120
		Physical memory (bytes) snapshot=444899328
		Virtual memory (bytes) snapshot=3840159744
		Total committed heap usage (bytes)=329777152
		Peak Map Physical memory (bytes)=276525056
		Peak Map Virtual memory (bytes)=1917837312
		Peak Reduce Physical memory (bytes)=168374272
		Peak Reduce Virtual memory (bytes)=1922322432
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=1373634
	File Output Format Counters 
		Bytes Written=136
2020-06-30 18:51:55,205 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
java.lang.NumberFormatException: multiple points
	at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:1914)
	at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110)
	at java.lang.Double.parseDouble(Double.java:538)
	at it.unipi.hadoop.Point.parseString(Point.java:51)
	at it.unipi.hadoop.Driver.getRandomCentroids(Driver.java:67)
	at it.unipi.hadoop.Driver.main(Driver.java:210)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:318)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:232)
2020-06-30 18:51:55,225 INFO client.RMProxy: Connecting to ResourceManager at hadoop-namenode/172.16.1.142:8032
2020-06-30 18:51:55,233 WARN mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2020-06-30 18:51:55,244 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1593532013608_0205
2020-06-30 18:51:55,267 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:51:55,300 INFO input.FileInputFormat: Total input files to process : 1
2020-06-30 18:51:55,328 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:51:55,383 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:51:55,397 INFO mapreduce.JobSubmitter: number of splits:1
2020-06-30 18:51:55,424 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:51:55,444 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1593532013608_0205
2020-06-30 18:51:55,444 INFO mapreduce.JobSubmitter: Executing with tokens: []
2020-06-30 18:51:55,660 INFO impl.YarnClientImpl: Submitted application application_1593532013608_0205
2020-06-30 18:51:55,663 INFO mapreduce.Job: The url to track the job: http://hadoop-namenode:8088/proxy/application_1593532013608_0205/
2020-06-30 18:51:55,663 INFO mapreduce.Job: Running job: job_1593532013608_0205
2020-06-30 18:51:57,949 INFO mapreduce.Job:  map 100% reduce 0%
2020-06-30 18:52:01,730 INFO mapreduce.Job: Job job_1593532013608_0205 running in uber mode : false
2020-06-30 18:52:01,731 INFO mapreduce.Job:  map 0% reduce 0%
2020-06-30 18:52:02,980 INFO mapreduce.Job:  map 100% reduce 100%
2020-06-30 18:52:02,987 INFO mapreduce.Job: Job job_1593532013608_0204 completed successfully
2020-06-30 18:52:03,024 INFO mapreduce.Job: Counters: 53
	File System Counters
		FILE: Number of bytes read=149
		FILE: Number of bytes written=436615
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=13738638
		HDFS: Number of bytes written=135
		HDFS: Number of read operations=8
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=1
		Launched reduce tasks=1
		Data-local map tasks=1
		Total time spent by all maps in occupied slots (ms)=5158
		Total time spent by all reduces in occupied slots (ms)=4034
		Total time spent by all map tasks (ms)=2579
		Total time spent by all reduce tasks (ms)=2017
		Total vcore-milliseconds taken by all map tasks=2579
		Total vcore-milliseconds taken by all reduce tasks=2017
		Total megabyte-milliseconds taken by all map tasks=660224
		Total megabyte-milliseconds taken by all reduce tasks=516352
	Map-Reduce Framework
		Map input records=100000
		Map output records=100000
		Map output bytes=14238502
		Map output materialized bytes=149
		Input split bytes=132
		Combine input records=100000
		Combine output records=1
		Reduce input groups=1
		Reduce shuffle bytes=149
		Reduce input records=1
		Reduce output records=1
		Spilled Records=2
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=117
		CPU time spent (ms)=1580
		Physical memory (bytes) snapshot=443813888
		Virtual memory (bytes) snapshot=3837714432
		Total committed heap usage (bytes)=330825728
		Peak Map Physical memory (bytes)=273244160
		Peak Map Virtual memory (bytes)=1914146816
		Peak Reduce Physical memory (bytes)=170569728
		Peak Reduce Virtual memory (bytes)=1923567616
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=13738506
	File Output Format Counters 
		Bytes Written=135
2020-06-30 18:52:03,033 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
java.lang.NumberFormatException: multiple points
	at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:1914)
	at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110)
	at java.lang.Double.parseDouble(Double.java:538)
	at it.unipi.hadoop.Point.parseString(Point.java:51)
	at it.unipi.hadoop.Driver.getRandomCentroids(Driver.java:67)
	at it.unipi.hadoop.Driver.main(Driver.java:210)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:318)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:232)
2020-06-30 18:52:03,085 INFO client.RMProxy: Connecting to ResourceManager at hadoop-namenode/172.16.1.142:8032
2020-06-30 18:52:03,093 WARN mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2020-06-30 18:52:03,105 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1593532013608_0206
2020-06-30 18:52:03,124 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:52:03,150 INFO input.FileInputFormat: Total input files to process : 1
2020-06-30 18:52:03,186 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:52:03,255 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:52:03,270 INFO mapreduce.JobSubmitter: number of splits:1
2020-06-30 18:52:03,296 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:52:03,316 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1593532013608_0206
2020-06-30 18:52:03,316 INFO mapreduce.JobSubmitter: Executing with tokens: []
2020-06-30 18:52:03,336 INFO impl.YarnClientImpl: Submitted application application_1593532013608_0206
2020-06-30 18:52:03,342 INFO mapreduce.Job: The url to track the job: http://hadoop-namenode:8088/proxy/application_1593532013608_0206/
2020-06-30 18:52:03,342 INFO mapreduce.Job: Running job: job_1593532013608_0206
2020-06-30 18:52:05,757 INFO mapreduce.Job:  map 100% reduce 0%
2020-06-30 18:52:09,434 INFO mapreduce.Job: Job job_1593532013608_0206 running in uber mode : false
2020-06-30 18:52:09,434 INFO mapreduce.Job:  map 0% reduce 0%
2020-06-30 18:52:10,781 INFO mapreduce.Job:  map 100% reduce 100%
2020-06-30 18:52:10,786 INFO mapreduce.Job: Job job_1593532013608_0205 completed successfully
2020-06-30 18:52:10,814 INFO mapreduce.Job: Counters: 53
	File System Counters
		FILE: Number of bytes read=154
		FILE: Number of bytes written=436623
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=1373765
		HDFS: Number of bytes written=140
		HDFS: Number of read operations=8
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=1
		Launched reduce tasks=1
		Data-local map tasks=1
		Total time spent by all maps in occupied slots (ms)=4452
		Total time spent by all reduces in occupied slots (ms)=4488
		Total time spent by all map tasks (ms)=2226
		Total time spent by all reduce tasks (ms)=2244
		Total vcore-milliseconds taken by all map tasks=2226
		Total vcore-milliseconds taken by all reduce tasks=2244
		Total megabyte-milliseconds taken by all map tasks=569856
		Total megabyte-milliseconds taken by all reduce tasks=574464
	Map-Reduce Framework
		Map input records=10000
		Map output records=10000
		Map output bytes=1423634
		Map output materialized bytes=154
		Input split bytes=131
		Combine input records=10000
		Combine output records=1
		Reduce input groups=1
		Reduce shuffle bytes=154
		Reduce input records=1
		Reduce output records=1
		Spilled Records=2
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=129
		CPU time spent (ms)=1110
		Physical memory (bytes) snapshot=442884096
		Virtual memory (bytes) snapshot=3836465152
		Total committed heap usage (bytes)=332922880
		Peak Map Physical memory (bytes)=273776640
		Peak Map Virtual memory (bytes)=1913974784
		Peak Reduce Physical memory (bytes)=169107456
		Peak Reduce Virtual memory (bytes)=1922490368
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=1373634
	File Output Format Counters 
		Bytes Written=140
2020-06-30 18:52:10,822 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
java.lang.NumberFormatException: For input string: "-0.5425114096919779-0.23626979012472638"
	at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043)
	at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110)
	at java.lang.Double.parseDouble(Double.java:538)
	at it.unipi.hadoop.Point.parseString(Point.java:51)
	at it.unipi.hadoop.Driver.getRandomCentroids(Driver.java:67)
	at it.unipi.hadoop.Driver.main(Driver.java:210)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:318)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:232)
2020-06-30 18:52:10,846 INFO client.RMProxy: Connecting to ResourceManager at hadoop-namenode/172.16.1.142:8032
2020-06-30 18:52:10,853 WARN mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2020-06-30 18:52:10,864 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1593532013608_0207
2020-06-30 18:52:10,878 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:52:10,908 INFO input.FileInputFormat: Total input files to process : 1
2020-06-30 18:52:10,936 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:52:10,989 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:52:11,006 INFO mapreduce.JobSubmitter: number of splits:1
2020-06-30 18:52:11,028 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:52:11,056 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1593532013608_0207
2020-06-30 18:52:11,056 INFO mapreduce.JobSubmitter: Executing with tokens: []
2020-06-30 18:52:11,076 INFO impl.YarnClientImpl: Submitted application application_1593532013608_0207
2020-06-30 18:52:11,083 INFO mapreduce.Job: The url to track the job: http://hadoop-namenode:8088/proxy/application_1593532013608_0207/
2020-06-30 18:52:11,084 INFO mapreduce.Job: Running job: job_1593532013608_0207
2020-06-30 18:52:15,467 INFO mapreduce.Job:  map 100% reduce 0%
2020-06-30 18:52:18,145 INFO mapreduce.Job: Job job_1593532013608_0207 running in uber mode : false
2020-06-30 18:52:18,145 INFO mapreduce.Job:  map 0% reduce 0%
2020-06-30 18:52:19,485 INFO mapreduce.Job:  map 100% reduce 100%
2020-06-30 18:52:20,493 INFO mapreduce.Job: Job job_1593532013608_0206 completed successfully
2020-06-30 18:52:20,529 INFO mapreduce.Job: Counters: 53
	File System Counters
		FILE: Number of bytes read=152
		FILE: Number of bytes written=436621
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=13738638
		HDFS: Number of bytes written=138
		HDFS: Number of read operations=8
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=1
		Launched reduce tasks=1
		Data-local map tasks=1
		Total time spent by all maps in occupied slots (ms)=6496
		Total time spent by all reduces in occupied slots (ms)=4424
		Total time spent by all map tasks (ms)=3248
		Total time spent by all reduce tasks (ms)=2212
		Total vcore-milliseconds taken by all map tasks=3248
		Total vcore-milliseconds taken by all reduce tasks=2212
		Total megabyte-milliseconds taken by all map tasks=831488
		Total megabyte-milliseconds taken by all reduce tasks=566272
	Map-Reduce Framework
		Map input records=100000
		Map output records=100000
		Map output bytes=14238502
		Map output materialized bytes=152
		Input split bytes=132
		Combine input records=100000
		Combine output records=1
		Reduce input groups=1
		Reduce shuffle bytes=152
		Reduce input records=1
		Reduce output records=1
		Spilled Records=2
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=140
		CPU time spent (ms)=1670
		Physical memory (bytes) snapshot=448516096
		Virtual memory (bytes) snapshot=3843706880
		Total committed heap usage (bytes)=330825728
		Peak Map Physical memory (bytes)=276606976
		Peak Map Virtual memory (bytes)=1918517248
		Peak Reduce Physical memory (bytes)=171909120
		Peak Reduce Virtual memory (bytes)=1925189632
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=13738506
	File Output Format Counters 
		Bytes Written=138
2020-06-30 18:52:20,540 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
java.lang.NumberFormatException: multiple points
	at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:1914)
	at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110)
	at java.lang.Double.parseDouble(Double.java:538)
	at it.unipi.hadoop.Point.parseString(Point.java:51)
	at it.unipi.hadoop.Driver.getRandomCentroids(Driver.java:67)
	at it.unipi.hadoop.Driver.main(Driver.java:210)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:318)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:232)
2020-06-30 18:52:20,565 INFO client.RMProxy: Connecting to ResourceManager at hadoop-namenode/172.16.1.142:8032
2020-06-30 18:52:20,577 WARN mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2020-06-30 18:52:20,589 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1593532013608_0208
2020-06-30 18:52:20,608 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:52:20,640 INFO input.FileInputFormat: Total input files to process : 1
2020-06-30 18:52:20,660 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:52:20,720 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:52:20,742 INFO mapreduce.JobSubmitter: number of splits:1
2020-06-30 18:52:20,765 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:52:20,781 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1593532013608_0208
2020-06-30 18:52:20,781 INFO mapreduce.JobSubmitter: Executing with tokens: []
2020-06-30 18:52:20,800 INFO impl.YarnClientImpl: Submitted application application_1593532013608_0208
2020-06-30 18:52:20,803 INFO mapreduce.Job: The url to track the job: http://hadoop-namenode:8088/proxy/application_1593532013608_0208/
2020-06-30 18:52:20,803 INFO mapreduce.Job: Running job: job_1593532013608_0208
2020-06-30 18:52:22,172 INFO mapreduce.Job:  map 100% reduce 0%
2020-06-30 18:52:27,905 INFO mapreduce.Job: Job job_1593532013608_0208 running in uber mode : false
2020-06-30 18:52:27,905 INFO mapreduce.Job:  map 0% reduce 0%
2020-06-30 18:52:29,205 INFO mapreduce.Job:  map 100% reduce 100%
2020-06-30 18:52:29,217 INFO mapreduce.Job: Job job_1593532013608_0207 completed successfully
2020-06-30 18:52:29,257 INFO mapreduce.Job: Counters: 53
	File System Counters
		FILE: Number of bytes read=151
		FILE: Number of bytes written=436585
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=1373765
		HDFS: Number of bytes written=137
		HDFS: Number of read operations=8
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=1
		Launched reduce tasks=1
		Rack-local map tasks=1
		Total time spent by all maps in occupied slots (ms)=4442
		Total time spent by all reduces in occupied slots (ms)=7876
		Total time spent by all map tasks (ms)=2221
		Total time spent by all reduce tasks (ms)=3938
		Total vcore-milliseconds taken by all map tasks=2221
		Total vcore-milliseconds taken by all reduce tasks=3938
		Total megabyte-milliseconds taken by all map tasks=568576
		Total megabyte-milliseconds taken by all reduce tasks=1008128
	Map-Reduce Framework
		Map input records=10000
		Map output records=10000
		Map output bytes=1423634
		Map output materialized bytes=151
		Input split bytes=131
		Combine input records=10000
		Combine output records=1
		Reduce input groups=1
		Reduce shuffle bytes=151
		Reduce input records=1
		Reduce output records=1
		Spilled Records=2
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=127
		CPU time spent (ms)=1240
		Physical memory (bytes) snapshot=442007552
		Virtual memory (bytes) snapshot=3836272640
		Total committed heap usage (bytes)=318242816
		Peak Map Physical memory (bytes)=273137664
		Peak Map Virtual memory (bytes)=1914093568
		Peak Reduce Physical memory (bytes)=168869888
		Peak Reduce Virtual memory (bytes)=1922179072
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=1373634
	File Output Format Counters 
		Bytes Written=137
2020-06-30 18:52:29,264 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
java.lang.NumberFormatException: multiple points
	at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:1914)
	at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110)
	at java.lang.Double.parseDouble(Double.java:538)
	at it.unipi.hadoop.Point.parseString(Point.java:51)
	at it.unipi.hadoop.Driver.getRandomCentroids(Driver.java:67)
	at it.unipi.hadoop.Driver.main(Driver.java:210)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:318)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:232)
2020-06-30 18:52:29,288 INFO client.RMProxy: Connecting to ResourceManager at hadoop-namenode/172.16.1.142:8032
2020-06-30 18:52:29,297 WARN mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2020-06-30 18:52:29,310 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1593532013608_0209
2020-06-30 18:52:29,332 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:52:29,359 INFO input.FileInputFormat: Total input files to process : 1
2020-06-30 18:52:29,389 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:52:29,464 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:52:29,484 INFO mapreduce.JobSubmitter: number of splits:1
2020-06-30 18:52:29,512 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:52:29,534 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1593532013608_0209
2020-06-30 18:52:29,534 INFO mapreduce.JobSubmitter: Executing with tokens: []
2020-06-30 18:52:29,548 INFO impl.YarnClientImpl: Submitted application application_1593532013608_0209
2020-06-30 18:52:29,550 INFO mapreduce.Job: The url to track the job: http://hadoop-namenode:8088/proxy/application_1593532013608_0209/
2020-06-30 18:52:29,551 INFO mapreduce.Job: Running job: job_1593532013608_0209
2020-06-30 18:52:31,929 INFO mapreduce.Job:  map 100% reduce 0%
2020-06-30 18:52:36,683 INFO mapreduce.Job: Job job_1593532013608_0209 running in uber mode : false
2020-06-30 18:52:36,684 INFO mapreduce.Job:  map 0% reduce 0%
2020-06-30 18:52:37,984 INFO mapreduce.Job:  map 100% reduce 100%
2020-06-30 18:52:39,006 INFO mapreduce.Job: Job job_1593532013608_0208 completed successfully
2020-06-30 18:52:39,154 INFO mapreduce.Job: Counters: 53
	File System Counters
		FILE: Number of bytes read=149
		FILE: Number of bytes written=436615
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=13738638
		HDFS: Number of bytes written=135
		HDFS: Number of read operations=8
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=1
		Launched reduce tasks=1
		Data-local map tasks=1
		Total time spent by all maps in occupied slots (ms)=5012
		Total time spent by all reduces in occupied slots (ms)=6644
		Total time spent by all map tasks (ms)=2506
		Total time spent by all reduce tasks (ms)=3322
		Total vcore-milliseconds taken by all map tasks=2506
		Total vcore-milliseconds taken by all reduce tasks=3322
		Total megabyte-milliseconds taken by all map tasks=641536
		Total megabyte-milliseconds taken by all reduce tasks=850432
	Map-Reduce Framework
		Map input records=100000
		Map output records=100000
		Map output bytes=14238502
		Map output materialized bytes=149
		Input split bytes=132
		Combine input records=100000
		Combine output records=1
		Reduce input groups=1
		Reduce shuffle bytes=149
		Reduce input records=1
		Reduce output records=1
		Spilled Records=2
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=161
		CPU time spent (ms)=1540
		Physical memory (bytes) snapshot=444891136
		Virtual memory (bytes) snapshot=3837259776
		Total committed heap usage (bytes)=328204288
		Peak Map Physical memory (bytes)=275615744
		Peak Map Virtual memory (bytes)=1915277312
		Peak Reduce Physical memory (bytes)=169275392
		Peak Reduce Virtual memory (bytes)=1921982464
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=13738506
	File Output Format Counters 
		Bytes Written=135
2020-06-30 18:52:39,185 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
java.lang.NumberFormatException: multiple points
	at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:1914)
	at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110)
	at java.lang.Double.parseDouble(Double.java:538)
	at it.unipi.hadoop.Point.parseString(Point.java:51)
	at it.unipi.hadoop.Driver.getRandomCentroids(Driver.java:67)
	at it.unipi.hadoop.Driver.main(Driver.java:210)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:318)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:232)
2020-06-30 18:52:39,259 INFO client.RMProxy: Connecting to ResourceManager at hadoop-namenode/172.16.1.142:8032
2020-06-30 18:52:39,279 WARN mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2020-06-30 18:52:39,298 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1593532013608_0210
2020-06-30 18:52:39,359 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:52:39,425 INFO input.FileInputFormat: Total input files to process : 1
2020-06-30 18:52:39,460 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:52:39,528 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:52:39,559 INFO mapreduce.JobSubmitter: number of splits:1
2020-06-30 18:52:39,593 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-06-30 18:52:39,622 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1593532013608_0210
2020-06-30 18:52:39,622 INFO mapreduce.JobSubmitter: Executing with tokens: []
2020-06-30 18:52:39,642 INFO impl.YarnClientImpl: Submitted application application_1593532013608_0210
2020-06-30 18:52:39,645 INFO mapreduce.Job: The url to track the job: http://hadoop-namenode:8088/proxy/application_1593532013608_0210/
2020-06-30 18:52:39,645 INFO mapreduce.Job: Running job: job_1593532013608_0210
2020-06-30 18:52:41,836 INFO mapreduce.Job:  map 100% reduce 0%
2020-06-30 18:52:43,662 INFO retry.RetryInvocationHandler: java.io.EOFException: End of File Exception between local host is: "hadoop-namenode/172.16.1.142"; destination host is: "hadoop-namenode":8032; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException, while invoking ApplicationClientProtocolPBClientImpl.getApplicationReport over null. Trying to failover immediately.
2020-06-30 18:52:44,663 INFO ipc.Client: Retrying connect to server: hadoop-namenode/172.16.1.142:8032. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-06-30 18:52:45,664 INFO ipc.Client: Retrying connect to server: hadoop-namenode/172.16.1.142:8032. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-06-30 18:52:45,855 INFO mapreduce.Job: Task Id : attempt_1593532013608_0209_r_000000_0, Status : FAILED
Error: java.lang.NullPointerException
	at sun.net.www.protocol.http.HttpURLConnection.disconnect(HttpURLConnection.java:2939)
	at org.apache.hadoop.mapreduce.task.reduce.Fetcher.closeConnection(Fetcher.java:255)
	at org.apache.hadoop.mapreduce.task.reduce.Fetcher.interrupt(Fetcher.java:216)
	at org.apache.hadoop.mapreduce.task.reduce.Fetcher.shutDown(Fetcher.java:224)
	at org.apache.hadoop.mapreduce.task.reduce.Shuffle.run(Shuffle.java:145)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:377)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)

2020-06-30 18:52:46,665 INFO ipc.Client: Retrying connect to server: hadoop-namenode/172.16.1.142:8032. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-06-30 18:52:47,665 INFO ipc.Client: Retrying connect to server: hadoop-namenode/172.16.1.142:8032. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-06-30 18:52:48,666 INFO ipc.Client: Retrying connect to server: hadoop-namenode/172.16.1.142:8032. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-06-30 18:52:49,666 INFO ipc.Client: Retrying connect to server: hadoop-namenode/172.16.1.142:8032. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-06-30 18:52:50,667 INFO ipc.Client: Retrying connect to server: hadoop-namenode/172.16.1.142:8032. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-06-30 18:52:51,667 INFO ipc.Client: Retrying connect to server: hadoop-namenode/172.16.1.142:8032. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-06-30 18:52:52,668 INFO ipc.Client: Retrying connect to server: hadoop-namenode/172.16.1.142:8032. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-06-30 18:52:53,669 INFO ipc.Client: Retrying connect to server: hadoop-namenode/172.16.1.142:8032. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-06-30 18:52:53,669 INFO retry.RetryInvocationHandler: java.net.ConnectException: Call From hadoop-namenode/172.16.1.142 to hadoop-namenode:8032 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking ApplicationClientProtocolPBClientImpl.getApplicationReport over null after 1 failover attempts. Trying to failover after sleeping for 36255ms.
